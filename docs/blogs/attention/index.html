<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Diwakar Basnet">
<meta name="dcterms.date" content="2025-06-05">

<title>Self-Attention – Diwakar Basnet</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Diwakar Basnet</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../index.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../blogs.html"> 
<span class="menu-text">Blog</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../resume.html"> 
<span class="menu-text">Resume</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools tools-wide">
    <a href="https://github.com/DiwakarBasnet" title="" class="quarto-navigation-tool px-1" aria-label=""><i class="bi bi-github"></i></a>
    <a href="https://www.linkedin.com/in/diwakar-basnet/" title="" class="quarto-navigation-tool px-1" aria-label=""><i class="bi bi-linkedin"></i></a>
    <a href="https://medium.com/@7diwakarbasnet" title="" class="quarto-navigation-tool px-1" aria-label=""><i class="bi bi-medium"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#mathematics-of-self-attention" id="toc-mathematics-of-self-attention" class="nav-link active" data-scroll-target="#mathematics-of-self-attention">Mathematics of self-attention</a>
  <ul class="collapse">
  <li><a href="#learnable-projections" id="toc-learnable-projections" class="nav-link" data-scroll-target="#learnable-projections"><strong>Learnable Projections</strong></a></li>
  <li><a href="#scaled-dot-product-attention" id="toc-scaled-dot-product-attention" class="nav-link" data-scroll-target="#scaled-dot-product-attention"><strong>Scaled Dot-Product Attention</strong></a></li>
  </ul></li>
  <li><a href="#self-attention-implementation-in-cpu" id="toc-self-attention-implementation-in-cpu" class="nav-link" data-scroll-target="#self-attention-implementation-in-cpu">Self-attention implementation in CPU</a></li>
  <li><a href="#naive-self-attention-implementation-in-cuda" id="toc-naive-self-attention-implementation-in-cuda" class="nav-link" data-scroll-target="#naive-self-attention-implementation-in-cuda">Naive self-attention implementation in CUDA</a></li>
  <li><a href="#optimized-self-attention-in-cuda" id="toc-optimized-self-attention-in-cuda" class="nav-link" data-scroll-target="#optimized-self-attention-in-cuda">Optimized self-attention in CUDA</a></li>
  <li><a href="#flash-attention-in-cuda" id="toc-flash-attention-in-cuda" class="nav-link" data-scroll-target="#flash-attention-in-cuda">Flash attention in CUDA</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Self-Attention</h1>
  <div class="quarto-categories">
    <div class="quarto-category">CUDA</div>
    <div class="quarto-category">Transformer</div>
  </div>
  </div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Diwakar Basnet </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">June 5, 2025</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<p>Since its introduction via the original transformer paper “Attention is all you need”, self-attention has become the corner stone of many state-of-the-art deep learning models, specially in the field of NLP. Self-attention mechanisms enable models to weigh different parts of input data differently, focusing on the most relevant information while performing a task. This mimics the human ability to selectively pay attention to certain aspects of our surroundings while filtering out distractions. Attention mechanisms have been instrumental in improving the performance of various AI models, particularly in sequence-to-sequence tasks.</p>
<p>In a transformer architecture, “query”, “key” and “value” are the fundamental components used for calculating the self-attention. In simple terms, suppose we have a book related to animals; query represents the question that one might have such as “what is the largest mammal on earth?”. Similarly, key represents the index or table of content in the book and value represents the actual answer that we obtain from the book, in this case “blue whale”. In technical terms;. <strong>Query:</strong> A query is a matrix that represents the current token of request in the input sequence. Each word in the sequence has an associated query vector. 2. <strong>Key:</strong> A key is also a matrix that represents the content or identity of each token. 3. <strong>Value:</strong> A value is the actual information of each token that can be passed along.</p>
<p>For each token in the input sequence; the self-attention model computes query vector from it, computes key and value vectors from every token in the sequence, calculates attention score by taking dot product of query and each key, applies softmax and finally computes the output as the weighted sum of the value vectors.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="https://sebastianraschka.com/images/blog/2023/self-attention-from-scratch/summary.png"><img src="self_attn.png" class="img-fluid figure-img" alt="Self-attention calculation"></a></p>
<figcaption>Self-attention calculation</figcaption>
</figure>
</div>
<section id="mathematics-of-self-attention" class="level2">
<h2 class="anchored" data-anchor-id="mathematics-of-self-attention">Mathematics of self-attention</h2>
<section id="learnable-projections" class="level3">
<h3 class="anchored" data-anchor-id="learnable-projections"><strong>Learnable Projections</strong></h3>
<p>Three trainable weight matrices, transform ( X ) into <strong>queries</strong> ( Q ), <strong>keys</strong> ( K ), and <strong>values</strong> ( V ):</p>
<p><span class="math display">\[Q = X \cdot W^Q, \quad W^Q \in \mathbb{R}^{d \times d_k}\]</span></p>
<p><span class="math display">\[K = X \cdot W^K, \quad W^K \in \mathbb{R}^{d \times d_k}\]</span></p>
<p><span class="math display">\[V = X \cdot W^V, \quad W^V \in \mathbb{R}^{d \times d_v}\]</span></p>
<p>Where: - <span class="math inline">\(( d_k )\)</span>: dimension of queries/keys - <span class="math inline">\(( d_v )\)</span>: dimension of values</p>
</section>
<section id="scaled-dot-product-attention" class="level3">
<h3 class="anchored" data-anchor-id="scaled-dot-product-attention"><strong>Scaled Dot-Product Attention</strong></h3>
<section id="step-1-compute-attention-scores" class="level4">
<h4 class="anchored" data-anchor-id="step-1-compute-attention-scores"><strong>Step 1:</strong> Compute Attention Scores</h4>
<p>Compute all pairwise “compatibility” scores between queries and keys <span class="math display">\[scores_{i,j} = Q_i \cdot K_j^T \in \mathbb{R}^{n \times n}\]</span></p>
</section>
<section id="step-2-scale-scores" class="level4">
<h4 class="anchored" data-anchor-id="step-2-scale-scores"><strong>Step 2:</strong> Scale Scores</h4>
<p>Because the magnitude of dot products grows with dimension <span class="math inline">\({d_k}\)</span>, we divide by <span class="math inline">\({\sqrt{d_k}}\)</span>: <span class="math display">\[scaled_scores_{i,j} = \frac{scores_{i,j}}{\sqrt{d_k}}\]</span></p>
</section>
<section id="step-3-softmax-normalization" class="level4">
<h4 class="anchored" data-anchor-id="step-3-softmax-normalization"><strong>Step 3:</strong> Softmax Normalization</h4>
<p>For each query i, we want to convert its scores scaled_scores into a probability distribution over the N keys: <span class="math display">\[A_{i,j} = softmax(scaled_scores_{i,j}), \quad A_{i,j} = \frac{e^{s_{i,j}}}{\sum_{k=1}^n e^{s_{i,k}}}\]</span></p>
</section>
<section id="step-4-weighted-sum-of-values" class="level4">
<h4 class="anchored" data-anchor-id="step-4-weighted-sum-of-values"><strong>Step 4:</strong> Weighted Sum of Values</h4>
<p><span class="math display">\[Output (O_i) = A_{i,j} \cdot V_j \in \mathbb{R}^{n \times d_v}\]</span></p>
</section>
</section>
</section>
<section id="self-attention-implementation-in-cpu" class="level2">
<h2 class="anchored" data-anchor-id="self-attention-implementation-in-cpu">Self-attention implementation in CPU</h2>
<p>Below is a step-by-step code walkthrough of C implementation that runs entirely on CPU.</p>
<p><strong>High-level structure:</strong> * Allocate and initialize input query/key/value matrices (query, key, value). * Compute the attention scores attentionScores = Q × Kᵀ (with loops). * Apply scaling + row-wise softmax to produce softmaxedScores. * Compute output = softmaxedScores × V.</p>
<div id="6f529652-0a94-4fed-b065-24d0f42d8ed8" class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co">#include &lt;stdio.h&gt;</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="co">#include &lt;stdlib.h&gt;</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="co">#include &lt;math.h&gt;</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="co">#define NUM_SAMPLES 2</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="co">#define FEATURE_DIMENSION 3</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>void printMatrix(<span class="bu">float</span> <span class="op">*</span>matrix, <span class="bu">int</span> row, <span class="bu">int</span> col) {</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> (<span class="bu">int</span> i <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> i <span class="op">&lt;</span> row<span class="op">;</span> i<span class="op">++</span>) {</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> (<span class="bu">int</span> j <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> j <span class="op">&lt;</span> col<span class="op">;</span> j<span class="op">++</span>) {</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>            printf(<span class="st">"</span><span class="sc">%f</span><span class="st"> "</span>, matrix[i <span class="op">*</span> col <span class="op">+</span> j])<span class="op">;</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>        }</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>        printf(<span class="st">"</span><span class="ch">\n</span><span class="st">"</span>)<span class="op">;</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a><span class="op">//</span> CPU Implementation of Attention</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>void transposeMatrix(<span class="bu">float</span> <span class="op">*</span>in_matrix, <span class="bu">float</span> <span class="op">*</span>out_matrix, <span class="bu">int</span> row, <span class="bu">int</span> col) {</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> (<span class="bu">int</span> i <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> i <span class="op">&lt;</span> row<span class="op">;</span> i<span class="op">++</span>) {</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> (<span class="bu">int</span> j <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> j <span class="op">&lt;</span> col<span class="op">;</span> j<span class="op">++</span>) {</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>            out_matrix[j <span class="op">*</span> row <span class="op">+</span> i] <span class="op">=</span> in_matrix[i <span class="op">*</span> col <span class="op">+</span> j]<span class="op">;</span></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>        }</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>void computeAttentionCPU(<span class="bu">float</span> <span class="op">*</span>query, <span class="bu">float</span> <span class="op">*</span>key, <span class="bu">float</span> <span class="op">*</span>value,</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>        <span class="bu">float</span> <span class="op">*</span>attentionScores, <span class="bu">float</span> <span class="op">*</span>output) {</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>    <span class="bu">float</span> <span class="op">*</span>transposeKey <span class="op">=</span> (<span class="bu">float</span><span class="op">*</span>)malloc(FEATURE_DIMENSION <span class="op">*</span> NUM_SAMPLES <span class="op">*</span> sizeof(<span class="bu">float</span>))<span class="op">;</span></span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>    transposeMatrix(key, transposeKey, NUM_SAMPLES, FEATURE_DIMENSION)<span class="op">;</span></span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>    <span class="bu">float</span> scalingFactor <span class="op">=</span> <span class="fl">1.0</span><span class="er">f</span> <span class="op">/</span> sqrt((<span class="bu">float</span>)FEATURE_DIMENSION)<span class="op">;</span></span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a>    <span class="op">//</span> Compute attention scores</span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> (<span class="bu">int</span> i <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> i <span class="op">&lt;</span> NUM_SAMPLES<span class="op">;</span> i<span class="op">++</span>) {</span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> (<span class="bu">int</span> j <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> j <span class="op">&lt;</span> NUM_SAMPLES<span class="op">;</span> j<span class="op">++</span>) {</span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> (<span class="bu">int</span> k <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> k <span class="op">&lt;</span> FEATURE_DIMENSION<span class="op">;</span> k<span class="op">++</span>) {</span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a>                attentionScores[i <span class="op">*</span> NUM_SAMPLES <span class="op">+</span> j] <span class="op">+=</span> query[i <span class="op">*</span> FEATURE_DIMENSION <span class="op">+</span> k] <span class="op">*</span> transposeKey[k <span class="op">*</span> NUM_SAMPLES <span class="op">+</span> j]<span class="op">;</span></span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a>            }</span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a>            attentionScores[i <span class="op">*</span> NUM_SAMPLES <span class="op">+</span> j] <span class="op">*=</span> scalingFactor<span class="op">;</span></span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a>        }</span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a>    <span class="op">//</span> Softmax row<span class="op">-</span>wise</span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> (<span class="bu">int</span> row <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> row <span class="op">&lt;</span> NUM_SAMPLES<span class="op">;</span> row<span class="op">++</span>) {</span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a>        <span class="bu">float</span> maxScore <span class="op">=</span> attentionScores[row <span class="op">*</span> NUM_SAMPLES]<span class="op">;</span></span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> (<span class="bu">int</span> col <span class="op">=</span> <span class="dv">1</span><span class="op">;</span> col <span class="op">&lt;</span> NUM_SAMPLES<span class="op">;</span> col<span class="op">++</span>) {</span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> (attentionScores[row <span class="op">*</span> NUM_SAMPLES <span class="op">+</span> col] <span class="op">&gt;</span> maxScore) {</span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true" tabindex="-1"></a>                maxScore <span class="op">=</span> attentionScores[row <span class="op">*</span> NUM_SAMPLES <span class="op">+</span> col]<span class="op">;</span></span>
<span id="cb1-49"><a href="#cb1-49" aria-hidden="true" tabindex="-1"></a>            }</span>
<span id="cb1-50"><a href="#cb1-50" aria-hidden="true" tabindex="-1"></a>        }</span>
<span id="cb1-51"><a href="#cb1-51" aria-hidden="true" tabindex="-1"></a>        <span class="bu">float</span> sumExp <span class="op">=</span> <span class="fl">0.0</span><span class="er">f</span><span class="op">;</span></span>
<span id="cb1-52"><a href="#cb1-52" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> (<span class="bu">int</span> col <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> col <span class="op">&lt;</span> NUM_SAMPLES<span class="op">;</span> col<span class="op">++</span>) {</span>
<span id="cb1-53"><a href="#cb1-53" aria-hidden="true" tabindex="-1"></a>            attentionScores[row <span class="op">*</span> NUM_SAMPLES <span class="op">+</span> col] <span class="op">=</span> expf(attentionScores[row <span class="op">*</span> NUM_SAMPLES <span class="op">+</span> col] <span class="op">-</span> maxScore)<span class="op">;</span></span>
<span id="cb1-54"><a href="#cb1-54" aria-hidden="true" tabindex="-1"></a>            sumExp <span class="op">+=</span> attentionScores[row <span class="op">*</span> NUM_SAMPLES <span class="op">+</span> col]<span class="op">;</span></span>
<span id="cb1-55"><a href="#cb1-55" aria-hidden="true" tabindex="-1"></a>        }</span>
<span id="cb1-56"><a href="#cb1-56" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> (<span class="bu">int</span> col <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> col <span class="op">&lt;</span> NUM_SAMPLES<span class="op">;</span> col<span class="op">++</span>) {</span>
<span id="cb1-57"><a href="#cb1-57" aria-hidden="true" tabindex="-1"></a>            attentionScores[row <span class="op">*</span> NUM_SAMPLES <span class="op">+</span> col] <span class="op">/=</span> sumExp<span class="op">;</span></span>
<span id="cb1-58"><a href="#cb1-58" aria-hidden="true" tabindex="-1"></a>        }</span>
<span id="cb1-59"><a href="#cb1-59" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb1-60"><a href="#cb1-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-61"><a href="#cb1-61" aria-hidden="true" tabindex="-1"></a>    <span class="op">//</span> Multiply by value matrix</span>
<span id="cb1-62"><a href="#cb1-62" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> (<span class="bu">int</span> i <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> i <span class="op">&lt;</span> NUM_SAMPLES<span class="op">;</span> i<span class="op">++</span>) {</span>
<span id="cb1-63"><a href="#cb1-63" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> (<span class="bu">int</span> j <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> j <span class="op">&lt;</span> FEATURE_DIMENSION<span class="op">;</span> j<span class="op">++</span>) {</span>
<span id="cb1-64"><a href="#cb1-64" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> (<span class="bu">int</span> k <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> k <span class="op">&lt;</span> NUM_SAMPLES<span class="op">;</span> k<span class="op">++</span>) {</span>
<span id="cb1-65"><a href="#cb1-65" aria-hidden="true" tabindex="-1"></a>                output[i <span class="op">*</span> FEATURE_DIMENSION <span class="op">+</span> j] <span class="op">+=</span> attentionScores[i <span class="op">*</span> NUM_SAMPLES <span class="op">+</span> k] <span class="op">*</span> value[k <span class="op">*</span> FEATURE_DIMENSION <span class="op">+</span> j]<span class="op">;</span></span>
<span id="cb1-66"><a href="#cb1-66" aria-hidden="true" tabindex="-1"></a>            }</span>
<span id="cb1-67"><a href="#cb1-67" aria-hidden="true" tabindex="-1"></a>        }</span>
<span id="cb1-68"><a href="#cb1-68" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb1-69"><a href="#cb1-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-70"><a href="#cb1-70" aria-hidden="true" tabindex="-1"></a>    free(transposeKey)<span class="op">;</span></span>
<span id="cb1-71"><a href="#cb1-71" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb1-72"><a href="#cb1-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-73"><a href="#cb1-73" aria-hidden="true" tabindex="-1"></a><span class="bu">int</span> main() {</span>
<span id="cb1-74"><a href="#cb1-74" aria-hidden="true" tabindex="-1"></a>    <span class="bu">float</span> query[NUM_SAMPLES <span class="op">*</span> FEATURE_DIMENSION] <span class="op">=</span> {</span>
<span id="cb1-75"><a href="#cb1-75" aria-hidden="true" tabindex="-1"></a>        <span class="fl">1.0</span><span class="er">f</span>, <span class="fl">0.0</span><span class="er">f</span>, <span class="op">-</span><span class="fl">1.0</span><span class="er">f</span>,</span>
<span id="cb1-76"><a href="#cb1-76" aria-hidden="true" tabindex="-1"></a>        <span class="fl">0.5</span><span class="er">f</span>, <span class="fl">0.5</span><span class="er">f</span>, <span class="fl">0.5</span><span class="er">f</span></span>
<span id="cb1-77"><a href="#cb1-77" aria-hidden="true" tabindex="-1"></a>    }<span class="op">;</span></span>
<span id="cb1-78"><a href="#cb1-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-79"><a href="#cb1-79" aria-hidden="true" tabindex="-1"></a>    <span class="bu">float</span> key[NUM_SAMPLES <span class="op">*</span> FEATURE_DIMENSION] <span class="op">=</span> {</span>
<span id="cb1-80"><a href="#cb1-80" aria-hidden="true" tabindex="-1"></a>        <span class="fl">1.0</span><span class="er">f</span>, <span class="fl">2.0</span><span class="er">f</span>, <span class="fl">3.0</span><span class="er">f</span>,</span>
<span id="cb1-81"><a href="#cb1-81" aria-hidden="true" tabindex="-1"></a>        <span class="fl">4.0</span><span class="er">f</span>, <span class="fl">5.0</span><span class="er">f</span>, <span class="fl">6.0</span><span class="er">f</span></span>
<span id="cb1-82"><a href="#cb1-82" aria-hidden="true" tabindex="-1"></a>    }<span class="op">;</span></span>
<span id="cb1-83"><a href="#cb1-83" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-84"><a href="#cb1-84" aria-hidden="true" tabindex="-1"></a>    <span class="bu">float</span> value[NUM_SAMPLES <span class="op">*</span> FEATURE_DIMENSION] <span class="op">=</span> {</span>
<span id="cb1-85"><a href="#cb1-85" aria-hidden="true" tabindex="-1"></a>        <span class="fl">1.0</span><span class="er">f</span>, <span class="fl">1.0</span><span class="er">f</span>, <span class="fl">1.0</span><span class="er">f</span>,</span>
<span id="cb1-86"><a href="#cb1-86" aria-hidden="true" tabindex="-1"></a>        <span class="fl">2.0</span><span class="er">f</span>, <span class="fl">2.0</span><span class="er">f</span>, <span class="fl">2.0</span><span class="er">f</span></span>
<span id="cb1-87"><a href="#cb1-87" aria-hidden="true" tabindex="-1"></a>    }<span class="op">;</span></span>
<span id="cb1-88"><a href="#cb1-88" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-89"><a href="#cb1-89" aria-hidden="true" tabindex="-1"></a>    <span class="bu">float</span><span class="op">*</span> output <span class="op">=</span> (<span class="bu">float</span><span class="op">*</span>)malloc(FEATURE_DIMENSION <span class="op">*</span> NUM_SAMPLES <span class="op">*</span> sizeof(<span class="bu">float</span>))<span class="op">;</span></span>
<span id="cb1-90"><a href="#cb1-90" aria-hidden="true" tabindex="-1"></a>    <span class="bu">float</span><span class="op">*</span> attentionScores <span class="op">=</span> (<span class="bu">float</span><span class="op">*</span>)malloc(NUM_SAMPLES <span class="op">*</span> NUM_SAMPLES <span class="op">*</span> sizeof(<span class="bu">float</span>))<span class="op">;</span></span>
<span id="cb1-91"><a href="#cb1-91" aria-hidden="true" tabindex="-1"></a>    computeAttentionCPU(query, key, value, attentionScores, output)<span class="op">;</span></span>
<span id="cb1-92"><a href="#cb1-92" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-93"><a href="#cb1-93" aria-hidden="true" tabindex="-1"></a>    printMatrix(output, NUM_SAMPLES, FEATURE_DIMENSION)<span class="op">;</span></span>
<span id="cb1-94"><a href="#cb1-94" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-95"><a href="#cb1-95" aria-hidden="true" tabindex="-1"></a>    free(output)<span class="op">;</span></span>
<span id="cb1-96"><a href="#cb1-96" aria-hidden="true" tabindex="-1"></a>    free(attentionScores)<span class="op">;</span></span>
<span id="cb1-97"><a href="#cb1-97" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-98"><a href="#cb1-98" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="dv">0</span><span class="op">;</span></span>
<span id="cb1-99"><a href="#cb1-99" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="naive-self-attention-implementation-in-cuda" class="level2">
<h2 class="anchored" data-anchor-id="naive-self-attention-implementation-in-cuda">Naive self-attention implementation in CUDA</h2>
<p>This naive approach simply offloads the same steps to GPU, but without any fancy shared-memory tiling. Each thread computes one element of the matrix multiply or one element of the output. This is not memory- or compute-optimal, but it’s the easiest way to see how we map loops to kernels.</p>
<div id="530b855b-2135-4945-a0da-0839f4199d9c" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co">#include &lt;stdio.h&gt;</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="co">#include &lt;cuda_runtime.h&gt;</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="co">#include &lt;math.h&gt;</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="co">#include &lt;stdlib.h&gt;</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="co">#define NUM_SAMPLES 5</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a><span class="co">#define FEATURE_DIMENSION 6</span></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>void printMatrix(<span class="bu">float</span> <span class="op">*</span>matrix, <span class="bu">int</span> row, <span class="bu">int</span> col) {</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> (<span class="bu">int</span> i <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> i <span class="op">&lt;</span> row<span class="op">;</span> i<span class="op">++</span>) {</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> (<span class="bu">int</span> j <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> j <span class="op">&lt;</span> col<span class="op">;</span> j<span class="op">++</span>) {</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>            printf(<span class="st">"</span><span class="sc">%.3f</span><span class="st"> "</span>, matrix[i <span class="op">*</span> col <span class="op">+</span> j])<span class="op">;</span></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>        }</span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>        printf(<span class="st">"</span><span class="ch">\n</span><span class="st">"</span>)<span class="op">;</span></span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a><span class="op">//</span> Kernel: Softmax</span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a>__global__ void softmaxKernel(<span class="bu">float</span> <span class="op">*</span>scoreMatrix, <span class="bu">float</span> <span class="op">*</span>softmaxMatrix) {</span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a>    <span class="bu">int</span> row <span class="op">=</span> blockIdx.y <span class="op">*</span> blockDim.y <span class="op">+</span> threadIdx.y<span class="op">;</span></span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> (row <span class="op">&lt;</span> NUM_SAMPLES) {</span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a>        <span class="bu">float</span> maxScore <span class="op">=</span> <span class="op">-</span><span class="fl">1e30</span><span class="er">f</span><span class="op">;</span></span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> (<span class="bu">int</span> col <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> col <span class="op">&lt;</span> NUM_SAMPLES<span class="op">;</span> <span class="op">++</span>col) {</span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a>            maxScore <span class="op">=</span> fmaxf(maxScore, scoreMatrix[row <span class="op">*</span> NUM_SAMPLES <span class="op">+</span> col])<span class="op">;</span></span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a>        }</span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a>        <span class="bu">float</span> sumExp <span class="op">=</span> <span class="fl">0.0</span><span class="er">f</span><span class="op">;</span></span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> (<span class="bu">int</span> col <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> col <span class="op">&lt;</span> NUM_SAMPLES<span class="op">;</span> <span class="op">++</span>col) {</span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true" tabindex="-1"></a>            softmaxMatrix[row <span class="op">*</span> NUM_SAMPLES <span class="op">+</span> col] <span class="op">=</span> </span>
<span id="cb2-29"><a href="#cb2-29" aria-hidden="true" tabindex="-1"></a>                expf(scoreMatrix[row <span class="op">*</span> NUM_SAMPLES <span class="op">+</span> col] <span class="op">-</span> maxScore)<span class="op">;</span></span>
<span id="cb2-30"><a href="#cb2-30" aria-hidden="true" tabindex="-1"></a>            sumExp <span class="op">+=</span> softmaxMatrix[row <span class="op">*</span> NUM_SAMPLES <span class="op">+</span> col]<span class="op">;</span></span>
<span id="cb2-31"><a href="#cb2-31" aria-hidden="true" tabindex="-1"></a>        }</span>
<span id="cb2-32"><a href="#cb2-32" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> (<span class="bu">int</span> col <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> col <span class="op">&lt;</span> NUM_SAMPLES<span class="op">;</span> <span class="op">++</span>col) {</span>
<span id="cb2-33"><a href="#cb2-33" aria-hidden="true" tabindex="-1"></a>            softmaxMatrix[row <span class="op">*</span> NUM_SAMPLES <span class="op">+</span> col] <span class="op">/=</span> sumExp<span class="op">;</span></span>
<span id="cb2-34"><a href="#cb2-34" aria-hidden="true" tabindex="-1"></a>        }</span>
<span id="cb2-35"><a href="#cb2-35" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb2-36"><a href="#cb2-36" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb2-37"><a href="#cb2-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-38"><a href="#cb2-38" aria-hidden="true" tabindex="-1"></a><span class="op">//</span> Kernel: QK<span class="op">^</span>T</span>
<span id="cb2-39"><a href="#cb2-39" aria-hidden="true" tabindex="-1"></a>__global__ void computeScoreKernel(<span class="bu">float</span> <span class="op">*</span>queryMatrix, <span class="bu">float</span> <span class="op">*</span>keyMatrix, <span class="bu">float</span> <span class="op">*</span>scoreMatrix) {</span>
<span id="cb2-40"><a href="#cb2-40" aria-hidden="true" tabindex="-1"></a>    <span class="bu">int</span> row <span class="op">=</span> blockIdx.y <span class="op">*</span> blockDim.y <span class="op">+</span> threadIdx.y<span class="op">;</span></span>
<span id="cb2-41"><a href="#cb2-41" aria-hidden="true" tabindex="-1"></a>    <span class="bu">int</span> col <span class="op">=</span> blockIdx.x <span class="op">*</span> blockDim.x <span class="op">+</span> threadIdx.x<span class="op">;</span></span>
<span id="cb2-42"><a href="#cb2-42" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> (row <span class="op">&lt;</span> NUM_SAMPLES <span class="op">&amp;&amp;</span> col <span class="op">&lt;</span> NUM_SAMPLES) {</span>
<span id="cb2-43"><a href="#cb2-43" aria-hidden="true" tabindex="-1"></a>        <span class="bu">float</span> score <span class="op">=</span> <span class="fl">0.0</span><span class="er">f</span><span class="op">;</span></span>
<span id="cb2-44"><a href="#cb2-44" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> (<span class="bu">int</span> d <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> d <span class="op">&lt;</span> FEATURE_DIMENSION<span class="op">;</span> <span class="op">++</span>d) {</span>
<span id="cb2-45"><a href="#cb2-45" aria-hidden="true" tabindex="-1"></a>            score <span class="op">+=</span> queryMatrix[row <span class="op">*</span> FEATURE_DIMENSION <span class="op">+</span> d] <span class="op">*</span></span>
<span id="cb2-46"><a href="#cb2-46" aria-hidden="true" tabindex="-1"></a>                keyMatrix[col <span class="op">*</span> FEATURE_DIMENSION <span class="op">+</span> d]<span class="op">;</span></span>
<span id="cb2-47"><a href="#cb2-47" aria-hidden="true" tabindex="-1"></a>        }</span>
<span id="cb2-48"><a href="#cb2-48" aria-hidden="true" tabindex="-1"></a>        scoreMatrix[row <span class="op">*</span> NUM_SAMPLES <span class="op">+</span> col] <span class="op">=</span> score <span class="op">/</span> sqrtf(static_cast<span class="op">&lt;</span><span class="bu">float</span><span class="op">&gt;</span>(FEATURE_DIMENSION))<span class="op">;</span></span>
<span id="cb2-49"><a href="#cb2-49" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb2-50"><a href="#cb2-50" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb2-51"><a href="#cb2-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-52"><a href="#cb2-52" aria-hidden="true" tabindex="-1"></a><span class="op">//</span> Kernel: Output <span class="op">=</span> Softmax(QK<span class="op">^</span>T) <span class="op">*</span> V</span>
<span id="cb2-53"><a href="#cb2-53" aria-hidden="true" tabindex="-1"></a>__global__ void computeOutputKernel(<span class="bu">float</span> <span class="op">*</span> softmaxMatrix, <span class="bu">float</span> <span class="op">*</span>valueMatrix, <span class="bu">float</span> <span class="op">*</span>outputMatrix) {</span>
<span id="cb2-54"><a href="#cb2-54" aria-hidden="true" tabindex="-1"></a>    <span class="bu">int</span> row <span class="op">=</span> blockIdx.y <span class="op">*</span> blockDim.y <span class="op">+</span> threadIdx.y<span class="op">;</span></span>
<span id="cb2-55"><a href="#cb2-55" aria-hidden="true" tabindex="-1"></a>    <span class="bu">int</span> col <span class="op">=</span> blockIdx.x <span class="op">*</span> blockDim.x <span class="op">+</span> threadIdx.x<span class="op">;</span></span>
<span id="cb2-56"><a href="#cb2-56" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> (row <span class="op">&lt;</span> NUM_SAMPLES <span class="op">&amp;&amp;</span> col <span class="op">&lt;</span> FEATURE_DIMENSION) {</span>
<span id="cb2-57"><a href="#cb2-57" aria-hidden="true" tabindex="-1"></a>        <span class="bu">float</span> result <span class="op">=</span> <span class="fl">0.0</span><span class="er">f</span><span class="op">;</span></span>
<span id="cb2-58"><a href="#cb2-58" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> (<span class="bu">int</span> k <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> k <span class="op">&lt;</span> NUM_SAMPLES<span class="op">;</span> <span class="op">++</span>k) {</span>
<span id="cb2-59"><a href="#cb2-59" aria-hidden="true" tabindex="-1"></a>            result <span class="op">+=</span> softmaxMatrix[row <span class="op">*</span> NUM_SAMPLES <span class="op">+</span> k] <span class="op">*</span></span>
<span id="cb2-60"><a href="#cb2-60" aria-hidden="true" tabindex="-1"></a>                valueMatrix[k <span class="op">*</span> FEATURE_DIMENSION <span class="op">+</span> col]<span class="op">;</span></span>
<span id="cb2-61"><a href="#cb2-61" aria-hidden="true" tabindex="-1"></a>        }</span>
<span id="cb2-62"><a href="#cb2-62" aria-hidden="true" tabindex="-1"></a>        outputMatrix[row <span class="op">*</span> FEATURE_DIMENSION <span class="op">+</span> col] <span class="op">=</span> result<span class="op">;</span></span>
<span id="cb2-63"><a href="#cb2-63" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb2-64"><a href="#cb2-64" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb2-65"><a href="#cb2-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-66"><a href="#cb2-66" aria-hidden="true" tabindex="-1"></a>void computeAttention(<span class="bu">float</span> <span class="op">*</span>queryMatrix_h, <span class="bu">float</span> <span class="op">*</span>keyMatrix_h, <span class="bu">float</span> <span class="op">*</span>valueMatrix_h, <span class="bu">float</span> <span class="op">*</span>attnMatrix_h) {</span>
<span id="cb2-67"><a href="#cb2-67" aria-hidden="true" tabindex="-1"></a>    <span class="bu">float</span> size <span class="op">=</span> NUM_SAMPLES <span class="op">*</span> FEATURE_DIMENSION <span class="op">*</span> sizeof(<span class="bu">float</span>)<span class="op">;</span></span>
<span id="cb2-68"><a href="#cb2-68" aria-hidden="true" tabindex="-1"></a>    <span class="bu">float</span> size_temp <span class="op">=</span> NUM_SAMPLES <span class="op">*</span> NUM_SAMPLES <span class="op">*</span> sizeof(<span class="bu">float</span>)<span class="op">;</span></span>
<span id="cb2-69"><a href="#cb2-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-70"><a href="#cb2-70" aria-hidden="true" tabindex="-1"></a>    <span class="bu">float</span> <span class="op">*</span>queryMatrix, <span class="op">*</span>keyMatrix, <span class="op">*</span>valueMatrix, <span class="op">*</span>attnMatrix, <span class="op">*</span>scoreMatrix, <span class="op">*</span>softmaxMatrix<span class="op">;</span></span>
<span id="cb2-71"><a href="#cb2-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-72"><a href="#cb2-72" aria-hidden="true" tabindex="-1"></a>    <span class="op">//</span> Device memory allocation</span>
<span id="cb2-73"><a href="#cb2-73" aria-hidden="true" tabindex="-1"></a>    cudaMalloc((void<span class="op">**</span>)<span class="op">&amp;</span>queryMatrix, size)<span class="op">;</span></span>
<span id="cb2-74"><a href="#cb2-74" aria-hidden="true" tabindex="-1"></a>    cudaMalloc((void<span class="op">**</span>)<span class="op">&amp;</span>keyMatrix, size)<span class="op">;</span></span>
<span id="cb2-75"><a href="#cb2-75" aria-hidden="true" tabindex="-1"></a>    cudaMalloc((void<span class="op">**</span>)<span class="op">&amp;</span>valueMatrix, size)<span class="op">;</span></span>
<span id="cb2-76"><a href="#cb2-76" aria-hidden="true" tabindex="-1"></a>    cudaMalloc((void<span class="op">**</span>)<span class="op">&amp;</span>attnMatrix, size)<span class="op">;</span></span>
<span id="cb2-77"><a href="#cb2-77" aria-hidden="true" tabindex="-1"></a>    cudaMalloc((void<span class="op">**</span>)<span class="op">&amp;</span>scoreMatrix, size_temp)<span class="op">;</span></span>
<span id="cb2-78"><a href="#cb2-78" aria-hidden="true" tabindex="-1"></a>    cudaMalloc((void<span class="op">**</span>)<span class="op">&amp;</span>softmaxMatrix, size_temp)<span class="op">;</span></span>
<span id="cb2-79"><a href="#cb2-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-80"><a href="#cb2-80" aria-hidden="true" tabindex="-1"></a>    cudaMemcpy(queryMatrix, queryMatrix_h, size, cudaMemcpyHostToDevice)<span class="op">;</span></span>
<span id="cb2-81"><a href="#cb2-81" aria-hidden="true" tabindex="-1"></a>    cudaMemcpy(keyMatrix, keyMatrix_h, size, cudaMemcpyHostToDevice)<span class="op">;</span></span>
<span id="cb2-82"><a href="#cb2-82" aria-hidden="true" tabindex="-1"></a>    cudaMemcpy(valueMatrix, valueMatrix_h, size, cudaMemcpyHostToDevice)<span class="op">;</span></span>
<span id="cb2-83"><a href="#cb2-83" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb2-84"><a href="#cb2-84" aria-hidden="true" tabindex="-1"></a>    <span class="op">//</span> Kernel initializations</span>
<span id="cb2-85"><a href="#cb2-85" aria-hidden="true" tabindex="-1"></a>    dim3 blockDim(<span class="dv">16</span>, <span class="dv">16</span>, <span class="dv">1</span>)<span class="op">;</span></span>
<span id="cb2-86"><a href="#cb2-86" aria-hidden="true" tabindex="-1"></a>    dim3 gridDim((NUM_SAMPLES<span class="op">+</span>blockDim.x<span class="op">-</span><span class="dv">1</span>)<span class="op">/</span>blockDim.x, (NUM_SAMPLES<span class="op">+</span>blockDim.y<span class="op">-</span><span class="dv">1</span>)<span class="op">/</span>blockDim.y, <span class="dv">1</span>)<span class="op">;</span></span>
<span id="cb2-87"><a href="#cb2-87" aria-hidden="true" tabindex="-1"></a>    computeScoreKernel<span class="op">&lt;&lt;&lt;</span>gridDim, blockDim<span class="op">&gt;&gt;&gt;</span>(queryMatrix, keyMatrix, scoreMatrix)<span class="op">;</span></span>
<span id="cb2-88"><a href="#cb2-88" aria-hidden="true" tabindex="-1"></a>    cudaDeviceSynchronize()<span class="op">;</span></span>
<span id="cb2-89"><a href="#cb2-89" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-90"><a href="#cb2-90" aria-hidden="true" tabindex="-1"></a>    dim3 softmaxBlockDim(<span class="dv">16</span>, <span class="dv">16</span>, <span class="dv">1</span>)<span class="op">;</span></span>
<span id="cb2-91"><a href="#cb2-91" aria-hidden="true" tabindex="-1"></a>    dim3 softmaxGridDim((NUM_SAMPLES<span class="op">+</span>softmaxBlockDim.x<span class="op">-</span><span class="dv">1</span>)<span class="op">/</span>softmaxBlockDim.x, (NUM_SAMPLES<span class="op">+</span>softmaxBlockDim.y<span class="op">-</span><span class="dv">1</span>)<span class="op">/</span>softmaxBlockDim.y, <span class="dv">1</span>)<span class="op">;</span></span>
<span id="cb2-92"><a href="#cb2-92" aria-hidden="true" tabindex="-1"></a>    softmaxKernel<span class="op">&lt;&lt;&lt;</span>softmaxGridDim, softmaxBlockDim<span class="op">&gt;&gt;&gt;</span>(scoreMatrix, softmaxMatrix)<span class="op">;</span></span>
<span id="cb2-93"><a href="#cb2-93" aria-hidden="true" tabindex="-1"></a>    cudaDeviceSynchronize()<span class="op">;</span></span>
<span id="cb2-94"><a href="#cb2-94" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-95"><a href="#cb2-95" aria-hidden="true" tabindex="-1"></a>    dim3 outputBlockDim(<span class="dv">16</span>, <span class="dv">16</span>, <span class="dv">1</span>)<span class="op">;</span></span>
<span id="cb2-96"><a href="#cb2-96" aria-hidden="true" tabindex="-1"></a>    dim3 outputGridDim((NUM_SAMPLES<span class="op">+</span>outputBlockDim.x<span class="op">-</span><span class="dv">1</span>)<span class="op">/</span>outputBlockDim.x, (NUM_SAMPLES<span class="op">+</span>outputBlockDim.y<span class="op">-</span><span class="dv">1</span>)<span class="op">/</span>outputBlockDim.y, <span class="dv">1</span>)<span class="op">;</span></span>
<span id="cb2-97"><a href="#cb2-97" aria-hidden="true" tabindex="-1"></a>    computeOutputKernel<span class="op">&lt;&lt;&lt;</span>outputGridDim, outputBlockDim<span class="op">&gt;&gt;&gt;</span>(softmaxMatrix, valueMatrix, attnMatrix)<span class="op">;</span></span>
<span id="cb2-98"><a href="#cb2-98" aria-hidden="true" tabindex="-1"></a>    cudaDeviceSynchronize()<span class="op">;</span></span>
<span id="cb2-99"><a href="#cb2-99" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-100"><a href="#cb2-100" aria-hidden="true" tabindex="-1"></a>    <span class="op">//</span> Copy output <span class="im">from</span> device to host</span>
<span id="cb2-101"><a href="#cb2-101" aria-hidden="true" tabindex="-1"></a>    cudaMemcpy(attnMatrix_h, attnMatrix, size, cudaMemcpyDeviceToHost)<span class="op">;</span></span>
<span id="cb2-102"><a href="#cb2-102" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-103"><a href="#cb2-103" aria-hidden="true" tabindex="-1"></a>    cudaFree(queryMatrix)<span class="op">;</span></span>
<span id="cb2-104"><a href="#cb2-104" aria-hidden="true" tabindex="-1"></a>    cudaFree(keyMatrix)<span class="op">;</span></span>
<span id="cb2-105"><a href="#cb2-105" aria-hidden="true" tabindex="-1"></a>    cudaFree(valueMatrix)<span class="op">;</span></span>
<span id="cb2-106"><a href="#cb2-106" aria-hidden="true" tabindex="-1"></a>    cudaFree(attnMatrix)<span class="op">;</span></span>
<span id="cb2-107"><a href="#cb2-107" aria-hidden="true" tabindex="-1"></a>    cudaFree(scoreMatrix)<span class="op">;</span></span>
<span id="cb2-108"><a href="#cb2-108" aria-hidden="true" tabindex="-1"></a>    cudaFree(softmaxMatrix)<span class="op">;</span></span>
<span id="cb2-109"><a href="#cb2-109" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb2-110"><a href="#cb2-110" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-111"><a href="#cb2-111" aria-hidden="true" tabindex="-1"></a><span class="bu">int</span> main() {</span>
<span id="cb2-112"><a href="#cb2-112" aria-hidden="true" tabindex="-1"></a>    <span class="bu">int</span> size <span class="op">=</span> NUM_SAMPLES <span class="op">*</span> FEATURE_DIMENSION <span class="op">*</span> sizeof(<span class="bu">float</span>)<span class="op">;</span></span>
<span id="cb2-113"><a href="#cb2-113" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-114"><a href="#cb2-114" aria-hidden="true" tabindex="-1"></a>    <span class="bu">float</span> <span class="op">*</span>queryMatrix <span class="op">=</span> (<span class="bu">float</span> <span class="op">*</span>)malloc(size)<span class="op">;</span></span>
<span id="cb2-115"><a href="#cb2-115" aria-hidden="true" tabindex="-1"></a>    <span class="bu">float</span> <span class="op">*</span>keyMatrix <span class="op">=</span> (<span class="bu">float</span> <span class="op">*</span>)malloc(size)<span class="op">;</span></span>
<span id="cb2-116"><a href="#cb2-116" aria-hidden="true" tabindex="-1"></a>    <span class="bu">float</span> <span class="op">*</span>valueMatrix <span class="op">=</span> (<span class="bu">float</span> <span class="op">*</span>)malloc(size)<span class="op">;</span></span>
<span id="cb2-117"><a href="#cb2-117" aria-hidden="true" tabindex="-1"></a>    <span class="bu">float</span> <span class="op">*</span>attnMatrix <span class="op">=</span> (<span class="bu">float</span> <span class="op">*</span>)malloc(size)<span class="op">;</span></span>
<span id="cb2-118"><a href="#cb2-118" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-119"><a href="#cb2-119" aria-hidden="true" tabindex="-1"></a>    <span class="op">//</span> Initialize matrix</span>
<span id="cb2-120"><a href="#cb2-120" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> (<span class="bu">int</span> i <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> i <span class="op">&lt;</span> NUM_SAMPLES <span class="op">*</span> FEATURE_DIMENSION<span class="op">;</span> i<span class="op">++</span>) {</span>
<span id="cb2-121"><a href="#cb2-121" aria-hidden="true" tabindex="-1"></a>        queryMatrix[i] <span class="op">=</span> (<span class="bu">float</span>)(rand() <span class="op">%</span> <span class="dv">50</span>)<span class="op">;</span></span>
<span id="cb2-122"><a href="#cb2-122" aria-hidden="true" tabindex="-1"></a>        keyMatrix[i] <span class="op">=</span> (<span class="bu">float</span>)(rand() <span class="op">%</span> <span class="dv">50</span>)<span class="op">;</span></span>
<span id="cb2-123"><a href="#cb2-123" aria-hidden="true" tabindex="-1"></a>        valueMatrix[i] <span class="op">=</span> (<span class="bu">float</span>)(rand() <span class="op">%</span> <span class="dv">50</span>)<span class="op">;</span></span>
<span id="cb2-124"><a href="#cb2-124" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb2-125"><a href="#cb2-125" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-126"><a href="#cb2-126" aria-hidden="true" tabindex="-1"></a>    printf(<span class="st">"</span><span class="ch">\n</span><span class="st">Query:</span><span class="ch">\n</span><span class="st">"</span>)<span class="op">;</span></span>
<span id="cb2-127"><a href="#cb2-127" aria-hidden="true" tabindex="-1"></a>    printMatrix(queryMatrix, NUM_SAMPLES, FEATURE_DIMENSION)<span class="op">;</span></span>
<span id="cb2-128"><a href="#cb2-128" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-129"><a href="#cb2-129" aria-hidden="true" tabindex="-1"></a>    printf(<span class="st">"</span><span class="ch">\n</span><span class="st">Key:</span><span class="ch">\n</span><span class="st">"</span>)<span class="op">;</span></span>
<span id="cb2-130"><a href="#cb2-130" aria-hidden="true" tabindex="-1"></a>    printMatrix(keyMatrix, NUM_SAMPLES, FEATURE_DIMENSION)<span class="op">;</span></span>
<span id="cb2-131"><a href="#cb2-131" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-132"><a href="#cb2-132" aria-hidden="true" tabindex="-1"></a>    printf(<span class="st">"</span><span class="ch">\n</span><span class="st">Value</span><span class="ch">\n</span><span class="st">"</span>)<span class="op">;</span></span>
<span id="cb2-133"><a href="#cb2-133" aria-hidden="true" tabindex="-1"></a>    printMatrix(valueMatrix, NUM_SAMPLES, FEATURE_DIMENSION)<span class="op">;</span></span>
<span id="cb2-134"><a href="#cb2-134" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-135"><a href="#cb2-135" aria-hidden="true" tabindex="-1"></a>    <span class="op">//</span> Attention calculation</span>
<span id="cb2-136"><a href="#cb2-136" aria-hidden="true" tabindex="-1"></a>    computeAttention(queryMatrix, keyMatrix, valueMatrix, attnMatrix)<span class="op">;</span></span>
<span id="cb2-137"><a href="#cb2-137" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-138"><a href="#cb2-138" aria-hidden="true" tabindex="-1"></a>    <span class="op">//</span> Print attention matrix</span>
<span id="cb2-139"><a href="#cb2-139" aria-hidden="true" tabindex="-1"></a>    printf(<span class="st">"</span><span class="ch">\n</span><span class="st">Attention matrix;\:</span><span class="ch">\n</span><span class="st">"</span>)<span class="op">;</span></span>
<span id="cb2-140"><a href="#cb2-140" aria-hidden="true" tabindex="-1"></a>    printMatrix(attnMatrix, NUM_SAMPLES, FEATURE_DIMENSION)<span class="op">;</span></span>
<span id="cb2-141"><a href="#cb2-141" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-142"><a href="#cb2-142" aria-hidden="true" tabindex="-1"></a>    <span class="op">//</span> Free memory</span>
<span id="cb2-143"><a href="#cb2-143" aria-hidden="true" tabindex="-1"></a>    free(queryMatrix)<span class="op">;</span></span>
<span id="cb2-144"><a href="#cb2-144" aria-hidden="true" tabindex="-1"></a>    free(keyMatrix)<span class="op">;</span></span>
<span id="cb2-145"><a href="#cb2-145" aria-hidden="true" tabindex="-1"></a>    free(valueMatrix)<span class="op">;</span></span>
<span id="cb2-146"><a href="#cb2-146" aria-hidden="true" tabindex="-1"></a>    free(attnMatrix)<span class="op">;</span></span>
<span id="cb2-147"><a href="#cb2-147" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-148"><a href="#cb2-148" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="dv">0</span><span class="op">;</span></span>
<span id="cb2-149"><a href="#cb2-149" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-error">
<div class="ansi-escaped-output">
<pre><span class="ansi-cyan-fg">  Cell </span><span class="ansi-green-fg">In[2], line 22</span>
<span class="ansi-red-fg">    float maxScore = -1e30f;</span>
                         ^
<span class="ansi-red-fg">SyntaxError</span><span class="ansi-red-fg">:</span> invalid decimal literal
</pre>
</div>
</div>
</div>
</section>
<section id="optimized-self-attention-in-cuda" class="level2">
<h2 class="anchored" data-anchor-id="optimized-self-attention-in-cuda">Optimized self-attention in CUDA</h2>
<p>This approach uses tiled/shared-memory to speed up the 𝑄𝐾⊤ and the final softmax × V operations. By loading contiguous chunks (tiles) of the input matrices into shared memory, threads within a block can cooperatively reuse data, minimizing expensive global‐memory round trips.</p>
<div id="fbc04e1b-9866-43b0-8b19-44049608c852" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co">#include &lt;stdio.h&gt;</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="co">#include &lt;stdlib.h&gt;</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="co">#include &lt;math.h&gt;</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="co">#include &lt;cuda_runtime.h&gt;</span></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="co">#define NUM_SAMPLES 5</span></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a><span class="co">#define FEATURE_DIMENSION 6</span></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a><span class="co">#define TILE_WIDTH 16</span></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a><span class="op">//</span> Print utility</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>void printMatrix(const <span class="bu">float</span><span class="op">*</span> matrix, <span class="bu">int</span> rows, <span class="bu">int</span> cols) {</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> (<span class="bu">int</span> r <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> r <span class="op">&lt;</span> rows<span class="op">;</span> <span class="op">++</span>r) {</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> (<span class="bu">int</span> c <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> c <span class="op">&lt;</span> cols<span class="op">;</span> <span class="op">++</span>c) {</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>            printf(<span class="st">"</span><span class="sc">%.3f</span><span class="st"> "</span>, matrix[r <span class="op">*</span> cols <span class="op">+</span> c])<span class="op">;</span></span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>        }</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>        printf(<span class="st">"</span><span class="ch">\n</span><span class="st">"</span>)<span class="op">;</span></span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a><span class="op">//</span> Kernel: compute Q <span class="op">*</span> K<span class="op">^</span>T (scores)</span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a>__global__ void scoreKernel(</span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a>    const <span class="bu">float</span><span class="op">*</span> __restrict__ query,</span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a>    const <span class="bu">float</span><span class="op">*</span> __restrict__ keyT,</span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a>    <span class="bu">float</span><span class="op">*</span> __restrict__ score) {</span>
<span id="cb3-25"><a href="#cb3-25" aria-hidden="true" tabindex="-1"></a>    __shared__ <span class="bu">float</span> sharedQ[TILE_WIDTH][TILE_WIDTH]<span class="op">;</span></span>
<span id="cb3-26"><a href="#cb3-26" aria-hidden="true" tabindex="-1"></a>    __shared__ <span class="bu">float</span> sharedK[TILE_WIDTH][TILE_WIDTH]<span class="op">;</span></span>
<span id="cb3-27"><a href="#cb3-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-28"><a href="#cb3-28" aria-hidden="true" tabindex="-1"></a>    <span class="bu">int</span> tx <span class="op">=</span> threadIdx.x<span class="op">;</span></span>
<span id="cb3-29"><a href="#cb3-29" aria-hidden="true" tabindex="-1"></a>    <span class="bu">int</span> ty <span class="op">=</span> threadIdx.y<span class="op">;</span></span>
<span id="cb3-30"><a href="#cb3-30" aria-hidden="true" tabindex="-1"></a>    <span class="bu">int</span> bx <span class="op">=</span> blockIdx.x<span class="op">;</span></span>
<span id="cb3-31"><a href="#cb3-31" aria-hidden="true" tabindex="-1"></a>    <span class="bu">int</span> by <span class="op">=</span> blockIdx.y<span class="op">;</span></span>
<span id="cb3-32"><a href="#cb3-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-33"><a href="#cb3-33" aria-hidden="true" tabindex="-1"></a>    <span class="bu">int</span> col <span class="op">=</span> bx <span class="op">*</span> TILE_WIDTH <span class="op">+</span> tx<span class="op">;</span></span>
<span id="cb3-34"><a href="#cb3-34" aria-hidden="true" tabindex="-1"></a>    <span class="bu">int</span> row <span class="op">=</span> by <span class="op">*</span> TILE_WIDTH <span class="op">+</span> ty<span class="op">;</span></span>
<span id="cb3-35"><a href="#cb3-35" aria-hidden="true" tabindex="-1"></a>    <span class="bu">float</span> acc <span class="op">=</span> <span class="fl">0.0</span><span class="er">f</span><span class="op">;</span></span>
<span id="cb3-36"><a href="#cb3-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-37"><a href="#cb3-37" aria-hidden="true" tabindex="-1"></a>    <span class="bu">int</span> phases <span class="op">=</span> (FEATURE_DIMENSION <span class="op">+</span> TILE_WIDTH <span class="op">-</span> <span class="dv">1</span>) <span class="op">/</span> TILE_WIDTH<span class="op">;</span></span>
<span id="cb3-38"><a href="#cb3-38" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> (<span class="bu">int</span> p <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> p <span class="op">&lt;</span> phases<span class="op">;</span> <span class="op">++</span>p) {</span>
<span id="cb3-39"><a href="#cb3-39" aria-hidden="true" tabindex="-1"></a>        <span class="bu">int</span> qCol <span class="op">=</span> p <span class="op">*</span> TILE_WIDTH <span class="op">+</span> tx<span class="op">;</span></span>
<span id="cb3-40"><a href="#cb3-40" aria-hidden="true" tabindex="-1"></a>        <span class="bu">int</span> kRow <span class="op">=</span> p <span class="op">*</span> TILE_WIDTH <span class="op">+</span> ty<span class="op">;</span></span>
<span id="cb3-41"><a href="#cb3-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-42"><a href="#cb3-42" aria-hidden="true" tabindex="-1"></a>        <span class="op">//</span> Load Q tile</span>
<span id="cb3-43"><a href="#cb3-43" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> (row <span class="op">&lt;</span> NUM_SAMPLES <span class="op">&amp;&amp;</span> qCol <span class="op">&lt;</span> FEATURE_DIMENSION)</span>
<span id="cb3-44"><a href="#cb3-44" aria-hidden="true" tabindex="-1"></a>            sharedQ[ty][tx] <span class="op">=</span> query[row <span class="op">*</span> FEATURE_DIMENSION <span class="op">+</span> qCol]<span class="op">;</span></span>
<span id="cb3-45"><a href="#cb3-45" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span></span>
<span id="cb3-46"><a href="#cb3-46" aria-hidden="true" tabindex="-1"></a>            sharedQ[ty][tx] <span class="op">=</span> <span class="fl">0.0</span><span class="er">f</span><span class="op">;</span></span>
<span id="cb3-47"><a href="#cb3-47" aria-hidden="true" tabindex="-1"></a>        <span class="op">//</span> Load K<span class="op">^</span>T tile</span>
<span id="cb3-48"><a href="#cb3-48" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> (col <span class="op">&lt;</span> NUM_SAMPLES <span class="op">&amp;&amp;</span> kRow <span class="op">&lt;</span> FEATURE_DIMENSION)</span>
<span id="cb3-49"><a href="#cb3-49" aria-hidden="true" tabindex="-1"></a>            sharedK[ty][tx] <span class="op">=</span> keyT[kRow <span class="op">*</span> NUM_SAMPLES <span class="op">+</span> col]<span class="op">;</span></span>
<span id="cb3-50"><a href="#cb3-50" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span></span>
<span id="cb3-51"><a href="#cb3-51" aria-hidden="true" tabindex="-1"></a>            sharedK[ty][tx] <span class="op">=</span> <span class="fl">0.0</span><span class="er">f</span><span class="op">;</span></span>
<span id="cb3-52"><a href="#cb3-52" aria-hidden="true" tabindex="-1"></a>        __syncthreads()<span class="op">;</span></span>
<span id="cb3-53"><a href="#cb3-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-54"><a href="#cb3-54" aria-hidden="true" tabindex="-1"></a>        <span class="op">//</span> Dot<span class="op">-</span>product</span>
<span id="cb3-55"><a href="#cb3-55" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> (<span class="bu">int</span> i <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> i <span class="op">&lt;</span> TILE_WIDTH<span class="op">;</span> <span class="op">++</span>i) {</span>
<span id="cb3-56"><a href="#cb3-56" aria-hidden="true" tabindex="-1"></a>            acc <span class="op">+=</span> sharedQ[ty][i] <span class="op">*</span> sharedK[i][tx]<span class="op">;</span></span>
<span id="cb3-57"><a href="#cb3-57" aria-hidden="true" tabindex="-1"></a>        }</span>
<span id="cb3-58"><a href="#cb3-58" aria-hidden="true" tabindex="-1"></a>        __syncthreads()<span class="op">;</span></span>
<span id="cb3-59"><a href="#cb3-59" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb3-60"><a href="#cb3-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-61"><a href="#cb3-61" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> (row <span class="op">&lt;</span> NUM_SAMPLES <span class="op">&amp;&amp;</span> col <span class="op">&lt;</span> NUM_SAMPLES) {</span>
<span id="cb3-62"><a href="#cb3-62" aria-hidden="true" tabindex="-1"></a>        score[row <span class="op">*</span> NUM_SAMPLES <span class="op">+</span> col] <span class="op">=</span> acc <span class="op">/</span> sqrtf((<span class="bu">float</span>)FEATURE_DIMENSION)<span class="op">;</span></span>
<span id="cb3-63"><a href="#cb3-63" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb3-64"><a href="#cb3-64" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb3-65"><a href="#cb3-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-66"><a href="#cb3-66" aria-hidden="true" tabindex="-1"></a><span class="op">//</span> Kernel: row<span class="op">-</span>wise softmax</span>
<span id="cb3-67"><a href="#cb3-67" aria-hidden="true" tabindex="-1"></a>__global__ void softmaxKernel(</span>
<span id="cb3-68"><a href="#cb3-68" aria-hidden="true" tabindex="-1"></a>    const <span class="bu">float</span><span class="op">*</span> __restrict__ score,</span>
<span id="cb3-69"><a href="#cb3-69" aria-hidden="true" tabindex="-1"></a>    <span class="bu">float</span><span class="op">*</span> __restrict__ softmax) {</span>
<span id="cb3-70"><a href="#cb3-70" aria-hidden="true" tabindex="-1"></a>    <span class="bu">int</span> row <span class="op">=</span> blockIdx.y <span class="op">*</span> blockDim.y <span class="op">+</span> threadIdx.y<span class="op">;</span></span>
<span id="cb3-71"><a href="#cb3-71" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> (row <span class="op">&lt;</span> NUM_SAMPLES) {</span>
<span id="cb3-72"><a href="#cb3-72" aria-hidden="true" tabindex="-1"></a>        <span class="bu">float</span> maxv <span class="op">=</span> <span class="op">-</span><span class="fl">1e30</span><span class="er">f</span><span class="op">;</span></span>
<span id="cb3-73"><a href="#cb3-73" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> (<span class="bu">int</span> c <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> c <span class="op">&lt;</span> NUM_SAMPLES<span class="op">;</span> <span class="op">++</span>c)</span>
<span id="cb3-74"><a href="#cb3-74" aria-hidden="true" tabindex="-1"></a>            maxv <span class="op">=</span> fmaxf(maxv, score[row <span class="op">*</span> NUM_SAMPLES <span class="op">+</span> c])<span class="op">;</span></span>
<span id="cb3-75"><a href="#cb3-75" aria-hidden="true" tabindex="-1"></a>        <span class="bu">float</span> <span class="bu">sum</span> <span class="op">=</span> <span class="fl">0.0</span><span class="er">f</span><span class="op">;</span></span>
<span id="cb3-76"><a href="#cb3-76" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> (<span class="bu">int</span> c <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> c <span class="op">&lt;</span> NUM_SAMPLES<span class="op">;</span> <span class="op">++</span>c) {</span>
<span id="cb3-77"><a href="#cb3-77" aria-hidden="true" tabindex="-1"></a>            <span class="bu">float</span> e <span class="op">=</span> expf(score[row <span class="op">*</span> NUM_SAMPLES <span class="op">+</span> c] <span class="op">-</span> maxv)<span class="op">;</span></span>
<span id="cb3-78"><a href="#cb3-78" aria-hidden="true" tabindex="-1"></a>            softmax[row <span class="op">*</span> NUM_SAMPLES <span class="op">+</span> c] <span class="op">=</span> e<span class="op">;</span></span>
<span id="cb3-79"><a href="#cb3-79" aria-hidden="true" tabindex="-1"></a>            <span class="bu">sum</span> <span class="op">+=</span> e<span class="op">;</span></span>
<span id="cb3-80"><a href="#cb3-80" aria-hidden="true" tabindex="-1"></a>        }</span>
<span id="cb3-81"><a href="#cb3-81" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> (<span class="bu">int</span> c <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> c <span class="op">&lt;</span> NUM_SAMPLES<span class="op">;</span> <span class="op">++</span>c)</span>
<span id="cb3-82"><a href="#cb3-82" aria-hidden="true" tabindex="-1"></a>            softmax[row <span class="op">*</span> NUM_SAMPLES <span class="op">+</span> c] <span class="op">/=</span> <span class="bu">sum</span><span class="op">;</span></span>
<span id="cb3-83"><a href="#cb3-83" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb3-84"><a href="#cb3-84" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb3-85"><a href="#cb3-85" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-86"><a href="#cb3-86" aria-hidden="true" tabindex="-1"></a><span class="op">//</span> Kernel: softmax <span class="op">*</span> V</span>
<span id="cb3-87"><a href="#cb3-87" aria-hidden="true" tabindex="-1"></a>__global__ void outputKernel(</span>
<span id="cb3-88"><a href="#cb3-88" aria-hidden="true" tabindex="-1"></a>    const <span class="bu">float</span><span class="op">*</span> __restrict__ softmax,</span>
<span id="cb3-89"><a href="#cb3-89" aria-hidden="true" tabindex="-1"></a>    const <span class="bu">float</span><span class="op">*</span> __restrict__ value,</span>
<span id="cb3-90"><a href="#cb3-90" aria-hidden="true" tabindex="-1"></a>    <span class="bu">float</span><span class="op">*</span> __restrict__ output) {</span>
<span id="cb3-91"><a href="#cb3-91" aria-hidden="true" tabindex="-1"></a>    __shared__ <span class="bu">float</span> sharedS[TILE_WIDTH][TILE_WIDTH]<span class="op">;</span></span>
<span id="cb3-92"><a href="#cb3-92" aria-hidden="true" tabindex="-1"></a>    __shared__ <span class="bu">float</span> sharedV[TILE_WIDTH][TILE_WIDTH]<span class="op">;</span></span>
<span id="cb3-93"><a href="#cb3-93" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-94"><a href="#cb3-94" aria-hidden="true" tabindex="-1"></a>    <span class="bu">int</span> tx <span class="op">=</span> threadIdx.x<span class="op">;</span></span>
<span id="cb3-95"><a href="#cb3-95" aria-hidden="true" tabindex="-1"></a>    <span class="bu">int</span> ty <span class="op">=</span> threadIdx.y<span class="op">;</span></span>
<span id="cb3-96"><a href="#cb3-96" aria-hidden="true" tabindex="-1"></a>    <span class="bu">int</span> bx <span class="op">=</span> blockIdx.x<span class="op">;</span></span>
<span id="cb3-97"><a href="#cb3-97" aria-hidden="true" tabindex="-1"></a>    <span class="bu">int</span> by <span class="op">=</span> blockIdx.y<span class="op">;</span></span>
<span id="cb3-98"><a href="#cb3-98" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-99"><a href="#cb3-99" aria-hidden="true" tabindex="-1"></a>    <span class="bu">int</span> col <span class="op">=</span> bx <span class="op">*</span> TILE_WIDTH <span class="op">+</span> tx<span class="op">;</span></span>
<span id="cb3-100"><a href="#cb3-100" aria-hidden="true" tabindex="-1"></a>    <span class="bu">int</span> row <span class="op">=</span> by <span class="op">*</span> TILE_WIDTH <span class="op">+</span> ty<span class="op">;</span></span>
<span id="cb3-101"><a href="#cb3-101" aria-hidden="true" tabindex="-1"></a>    <span class="bu">float</span> acc <span class="op">=</span> <span class="fl">0.0</span><span class="er">f</span><span class="op">;</span></span>
<span id="cb3-102"><a href="#cb3-102" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-103"><a href="#cb3-103" aria-hidden="true" tabindex="-1"></a>    <span class="bu">int</span> phases <span class="op">=</span> (NUM_SAMPLES <span class="op">+</span> TILE_WIDTH <span class="op">-</span> <span class="dv">1</span>) <span class="op">/</span> TILE_WIDTH<span class="op">;</span></span>
<span id="cb3-104"><a href="#cb3-104" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> (<span class="bu">int</span> p <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> p <span class="op">&lt;</span> phases<span class="op">;</span> <span class="op">++</span>p) {</span>
<span id="cb3-105"><a href="#cb3-105" aria-hidden="true" tabindex="-1"></a>        <span class="bu">int</span> sCol <span class="op">=</span> p <span class="op">*</span> TILE_WIDTH <span class="op">+</span> tx<span class="op">;</span></span>
<span id="cb3-106"><a href="#cb3-106" aria-hidden="true" tabindex="-1"></a>        <span class="bu">int</span> vRow <span class="op">=</span> p <span class="op">*</span> TILE_WIDTH <span class="op">+</span> ty<span class="op">;</span></span>
<span id="cb3-107"><a href="#cb3-107" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-108"><a href="#cb3-108" aria-hidden="true" tabindex="-1"></a>        <span class="op">//</span> Load softmax tile</span>
<span id="cb3-109"><a href="#cb3-109" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> (row <span class="op">&lt;</span> NUM_SAMPLES <span class="op">&amp;&amp;</span> sCol <span class="op">&lt;</span> NUM_SAMPLES)</span>
<span id="cb3-110"><a href="#cb3-110" aria-hidden="true" tabindex="-1"></a>            sharedS[ty][tx] <span class="op">=</span> softmax[row <span class="op">*</span> NUM_SAMPLES <span class="op">+</span> sCol]<span class="op">;</span></span>
<span id="cb3-111"><a href="#cb3-111" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span></span>
<span id="cb3-112"><a href="#cb3-112" aria-hidden="true" tabindex="-1"></a>            sharedS[ty][tx] <span class="op">=</span> <span class="fl">0.0</span><span class="er">f</span><span class="op">;</span></span>
<span id="cb3-113"><a href="#cb3-113" aria-hidden="true" tabindex="-1"></a>        <span class="op">//</span> Load V tile</span>
<span id="cb3-114"><a href="#cb3-114" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> (vRow <span class="op">&lt;</span> NUM_SAMPLES <span class="op">&amp;&amp;</span> col <span class="op">&lt;</span> FEATURE_DIMENSION)</span>
<span id="cb3-115"><a href="#cb3-115" aria-hidden="true" tabindex="-1"></a>            sharedV[ty][tx] <span class="op">=</span> value[vRow <span class="op">*</span> FEATURE_DIMENSION <span class="op">+</span> col]<span class="op">;</span></span>
<span id="cb3-116"><a href="#cb3-116" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span></span>
<span id="cb3-117"><a href="#cb3-117" aria-hidden="true" tabindex="-1"></a>            sharedV[ty][tx] <span class="op">=</span> <span class="fl">0.0</span><span class="er">f</span><span class="op">;</span></span>
<span id="cb3-118"><a href="#cb3-118" aria-hidden="true" tabindex="-1"></a>        __syncthreads()<span class="op">;</span></span>
<span id="cb3-119"><a href="#cb3-119" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-120"><a href="#cb3-120" aria-hidden="true" tabindex="-1"></a>        <span class="op">//</span> Dot<span class="op">-</span>product</span>
<span id="cb3-121"><a href="#cb3-121" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> (<span class="bu">int</span> i <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> i <span class="op">&lt;</span> TILE_WIDTH<span class="op">;</span> <span class="op">++</span>i) {</span>
<span id="cb3-122"><a href="#cb3-122" aria-hidden="true" tabindex="-1"></a>            acc <span class="op">+=</span> sharedS[ty][i] <span class="op">*</span> sharedV[i][tx]<span class="op">;</span></span>
<span id="cb3-123"><a href="#cb3-123" aria-hidden="true" tabindex="-1"></a>        }</span>
<span id="cb3-124"><a href="#cb3-124" aria-hidden="true" tabindex="-1"></a>        __syncthreads()<span class="op">;</span></span>
<span id="cb3-125"><a href="#cb3-125" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb3-126"><a href="#cb3-126" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-127"><a href="#cb3-127" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> (row <span class="op">&lt;</span> NUM_SAMPLES <span class="op">&amp;&amp;</span> col <span class="op">&lt;</span> FEATURE_DIMENSION) {</span>
<span id="cb3-128"><a href="#cb3-128" aria-hidden="true" tabindex="-1"></a>        output[row <span class="op">*</span> FEATURE_DIMENSION <span class="op">+</span> col] <span class="op">=</span> acc<span class="op">;</span></span>
<span id="cb3-129"><a href="#cb3-129" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb3-130"><a href="#cb3-130" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb3-131"><a href="#cb3-131" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-132"><a href="#cb3-132" aria-hidden="true" tabindex="-1"></a><span class="op">//</span> Host helper: transpose key</span>
<span id="cb3-133"><a href="#cb3-133" aria-hidden="true" tabindex="-1"></a>void transposeKey(const <span class="bu">float</span><span class="op">*</span> key, <span class="bu">float</span><span class="op">*</span> keyT) {</span>
<span id="cb3-134"><a href="#cb3-134" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> (<span class="bu">int</span> r <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> r <span class="op">&lt;</span> NUM_SAMPLES<span class="op">;</span> <span class="op">++</span>r)</span>
<span id="cb3-135"><a href="#cb3-135" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> (<span class="bu">int</span> c <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> c <span class="op">&lt;</span> FEATURE_DIMENSION<span class="op">;</span> <span class="op">++</span>c)</span>
<span id="cb3-136"><a href="#cb3-136" aria-hidden="true" tabindex="-1"></a>            keyT[c <span class="op">*</span> NUM_SAMPLES <span class="op">+</span> r] <span class="op">=</span> key[r <span class="op">*</span> FEATURE_DIMENSION <span class="op">+</span> c]<span class="op">;</span></span>
<span id="cb3-137"><a href="#cb3-137" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb3-138"><a href="#cb3-138" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-139"><a href="#cb3-139" aria-hidden="true" tabindex="-1"></a><span class="bu">int</span> main() {</span>
<span id="cb3-140"><a href="#cb3-140" aria-hidden="true" tabindex="-1"></a>    size_t qSize <span class="op">=</span> NUM_SAMPLES <span class="op">*</span> FEATURE_DIMENSION <span class="op">*</span> sizeof(<span class="bu">float</span>)<span class="op">;</span></span>
<span id="cb3-141"><a href="#cb3-141" aria-hidden="true" tabindex="-1"></a>    size_t kTSize <span class="op">=</span> NUM_SAMPLES <span class="op">*</span> FEATURE_DIMENSION <span class="op">*</span> sizeof(<span class="bu">float</span>)<span class="op">;</span></span>
<span id="cb3-142"><a href="#cb3-142" aria-hidden="true" tabindex="-1"></a>    size_t sSize <span class="op">=</span> NUM_SAMPLES <span class="op">*</span> NUM_SAMPLES <span class="op">*</span> sizeof(<span class="bu">float</span>)<span class="op">;</span></span>
<span id="cb3-143"><a href="#cb3-143" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-144"><a href="#cb3-144" aria-hidden="true" tabindex="-1"></a>    <span class="op">//</span> Host allocations</span>
<span id="cb3-145"><a href="#cb3-145" aria-hidden="true" tabindex="-1"></a>    <span class="bu">float</span> <span class="op">*</span>hQ <span class="op">=</span> (<span class="bu">float</span><span class="op">*</span>)malloc(qSize)<span class="op">;</span></span>
<span id="cb3-146"><a href="#cb3-146" aria-hidden="true" tabindex="-1"></a>    <span class="bu">float</span> <span class="op">*</span>hK <span class="op">=</span> (<span class="bu">float</span><span class="op">*</span>)malloc(qSize)<span class="op">;</span></span>
<span id="cb3-147"><a href="#cb3-147" aria-hidden="true" tabindex="-1"></a>    <span class="bu">float</span> <span class="op">*</span>hV <span class="op">=</span> (<span class="bu">float</span><span class="op">*</span>)malloc(qSize)<span class="op">;</span></span>
<span id="cb3-148"><a href="#cb3-148" aria-hidden="true" tabindex="-1"></a>    <span class="bu">float</span> <span class="op">*</span>hKT <span class="op">=</span> (<span class="bu">float</span><span class="op">*</span>)malloc(kTSize)<span class="op">;</span></span>
<span id="cb3-149"><a href="#cb3-149" aria-hidden="true" tabindex="-1"></a>    <span class="bu">float</span> <span class="op">*</span>hScore <span class="op">=</span> (<span class="bu">float</span><span class="op">*</span>)malloc(sSize)<span class="op">;</span></span>
<span id="cb3-150"><a href="#cb3-150" aria-hidden="true" tabindex="-1"></a>    <span class="bu">float</span> <span class="op">*</span>hSoftmax <span class="op">=</span> (<span class="bu">float</span><span class="op">*</span>)malloc(sSize)<span class="op">;</span></span>
<span id="cb3-151"><a href="#cb3-151" aria-hidden="true" tabindex="-1"></a>    <span class="bu">float</span> <span class="op">*</span>hOut <span class="op">=</span> (<span class="bu">float</span><span class="op">*</span>)malloc(qSize)<span class="op">;</span></span>
<span id="cb3-152"><a href="#cb3-152" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-153"><a href="#cb3-153" aria-hidden="true" tabindex="-1"></a>    <span class="op">//</span> Random init</span>
<span id="cb3-154"><a href="#cb3-154" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> (<span class="bu">int</span> i <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> i <span class="op">&lt;</span> NUM_SAMPLES <span class="op">*</span> FEATURE_DIMENSION<span class="op">;</span> <span class="op">++</span>i) {</span>
<span id="cb3-155"><a href="#cb3-155" aria-hidden="true" tabindex="-1"></a>        hQ[i] <span class="op">=</span> rand() <span class="op">%</span> <span class="dv">50</span><span class="op">;</span></span>
<span id="cb3-156"><a href="#cb3-156" aria-hidden="true" tabindex="-1"></a>        hK[i] <span class="op">=</span> rand() <span class="op">%</span> <span class="dv">50</span><span class="op">;</span></span>
<span id="cb3-157"><a href="#cb3-157" aria-hidden="true" tabindex="-1"></a>        hV[i] <span class="op">=</span> rand() <span class="op">%</span> <span class="dv">50</span><span class="op">;</span></span>
<span id="cb3-158"><a href="#cb3-158" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb3-159"><a href="#cb3-159" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-160"><a href="#cb3-160" aria-hidden="true" tabindex="-1"></a>    printf(<span class="st">"</span><span class="ch">\n</span><span class="st">Query:</span><span class="ch">\n</span><span class="st">"</span>)<span class="op">;</span> printMatrix(hQ, NUM_SAMPLES, FEATURE_DIMENSION)<span class="op">;</span></span>
<span id="cb3-161"><a href="#cb3-161" aria-hidden="true" tabindex="-1"></a>    printf(<span class="st">"</span><span class="ch">\n</span><span class="st">Key:</span><span class="ch">\n</span><span class="st">"</span>)<span class="op">;</span>   printMatrix(hK, NUM_SAMPLES, FEATURE_DIMENSION)<span class="op">;</span></span>
<span id="cb3-162"><a href="#cb3-162" aria-hidden="true" tabindex="-1"></a>    printf(<span class="st">"</span><span class="ch">\n</span><span class="st">Value:</span><span class="ch">\n</span><span class="st">"</span>)<span class="op">;</span> printMatrix(hV, NUM_SAMPLES, FEATURE_DIMENSION)<span class="op">;</span></span>
<span id="cb3-163"><a href="#cb3-163" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-164"><a href="#cb3-164" aria-hidden="true" tabindex="-1"></a>    <span class="op">//</span> Transpose key on host</span>
<span id="cb3-165"><a href="#cb3-165" aria-hidden="true" tabindex="-1"></a>    transposeKey(hK, hKT)<span class="op">;</span></span>
<span id="cb3-166"><a href="#cb3-166" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-167"><a href="#cb3-167" aria-hidden="true" tabindex="-1"></a>    <span class="op">//</span> Device allocations</span>
<span id="cb3-168"><a href="#cb3-168" aria-hidden="true" tabindex="-1"></a>    <span class="bu">float</span> <span class="op">*</span>dQ, <span class="op">*</span>dKT, <span class="op">*</span>dV, <span class="op">*</span>dScore, <span class="op">*</span>dSoftmax, <span class="op">*</span>dOut<span class="op">;</span></span>
<span id="cb3-169"><a href="#cb3-169" aria-hidden="true" tabindex="-1"></a>    cudaMalloc(<span class="op">&amp;</span>dQ, qSize)<span class="op">;</span></span>
<span id="cb3-170"><a href="#cb3-170" aria-hidden="true" tabindex="-1"></a>    cudaMalloc(<span class="op">&amp;</span>dKT, kTSize)<span class="op">;</span></span>
<span id="cb3-171"><a href="#cb3-171" aria-hidden="true" tabindex="-1"></a>    cudaMalloc(<span class="op">&amp;</span>dV, qSize)<span class="op">;</span></span>
<span id="cb3-172"><a href="#cb3-172" aria-hidden="true" tabindex="-1"></a>    cudaMalloc(<span class="op">&amp;</span>dScore, sSize)<span class="op">;</span></span>
<span id="cb3-173"><a href="#cb3-173" aria-hidden="true" tabindex="-1"></a>    cudaMalloc(<span class="op">&amp;</span>dSoftmax, sSize)<span class="op">;</span></span>
<span id="cb3-174"><a href="#cb3-174" aria-hidden="true" tabindex="-1"></a>    cudaMalloc(<span class="op">&amp;</span>dOut, qSize)<span class="op">;</span></span>
<span id="cb3-175"><a href="#cb3-175" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-176"><a href="#cb3-176" aria-hidden="true" tabindex="-1"></a>    <span class="op">//</span> Copy to device</span>
<span id="cb3-177"><a href="#cb3-177" aria-hidden="true" tabindex="-1"></a>    cudaMemcpy(dQ, hQ, qSize, cudaMemcpyHostToDevice)<span class="op">;</span></span>
<span id="cb3-178"><a href="#cb3-178" aria-hidden="true" tabindex="-1"></a>    cudaMemcpy(dKT, hKT, kTSize, cudaMemcpyHostToDevice)<span class="op">;</span></span>
<span id="cb3-179"><a href="#cb3-179" aria-hidden="true" tabindex="-1"></a>    cudaMemcpy(dV, hV, qSize, cudaMemcpyHostToDevice)<span class="op">;</span></span>
<span id="cb3-180"><a href="#cb3-180" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-181"><a href="#cb3-181" aria-hidden="true" tabindex="-1"></a>    <span class="op">//</span> Launch score kernel</span>
<span id="cb3-182"><a href="#cb3-182" aria-hidden="true" tabindex="-1"></a>    dim3 block(TILE_WIDTH, TILE_WIDTH)<span class="op">;</span></span>
<span id="cb3-183"><a href="#cb3-183" aria-hidden="true" tabindex="-1"></a>    dim3 gridScore((NUM_SAMPLES<span class="op">+</span>TILE_WIDTH<span class="op">-</span><span class="dv">1</span>)<span class="op">/</span>TILE_WIDTH,</span>
<span id="cb3-184"><a href="#cb3-184" aria-hidden="true" tabindex="-1"></a>                   (NUM_SAMPLES<span class="op">+</span>TILE_WIDTH<span class="op">-</span><span class="dv">1</span>)<span class="op">/</span>TILE_WIDTH)<span class="op">;</span></span>
<span id="cb3-185"><a href="#cb3-185" aria-hidden="true" tabindex="-1"></a>    scoreKernel<span class="op">&lt;&lt;&lt;</span>gridScore, block<span class="op">&gt;&gt;&gt;</span>(dQ, dKT, dScore)<span class="op">;</span></span>
<span id="cb3-186"><a href="#cb3-186" aria-hidden="true" tabindex="-1"></a>    cudaDeviceSynchronize()<span class="op">;</span></span>
<span id="cb3-187"><a href="#cb3-187" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-188"><a href="#cb3-188" aria-hidden="true" tabindex="-1"></a>    <span class="op">//</span> Softmax kernel</span>
<span id="cb3-189"><a href="#cb3-189" aria-hidden="true" tabindex="-1"></a>    dim3 gridSm((NUM_SAMPLES<span class="op">+</span>TILE_WIDTH<span class="op">-</span><span class="dv">1</span>)<span class="op">/</span>TILE_WIDTH, <span class="dv">1</span>)<span class="op">;</span></span>
<span id="cb3-190"><a href="#cb3-190" aria-hidden="true" tabindex="-1"></a>    softmaxKernel<span class="op">&lt;&lt;&lt;</span>gridSm, block<span class="op">&gt;&gt;&gt;</span>(dScore, dSoftmax)<span class="op">;</span></span>
<span id="cb3-191"><a href="#cb3-191" aria-hidden="true" tabindex="-1"></a>    cudaDeviceSynchronize()<span class="op">;</span></span>
<span id="cb3-192"><a href="#cb3-192" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-193"><a href="#cb3-193" aria-hidden="true" tabindex="-1"></a>    <span class="op">//</span> Output kernel</span>
<span id="cb3-194"><a href="#cb3-194" aria-hidden="true" tabindex="-1"></a>    dim3 gridOut((FEATURE_DIMENSION<span class="op">+</span>TILE_WIDTH<span class="op">-</span><span class="dv">1</span>)<span class="op">/</span>TILE_WIDTH,</span>
<span id="cb3-195"><a href="#cb3-195" aria-hidden="true" tabindex="-1"></a>                 (NUM_SAMPLES<span class="op">+</span>TILE_WIDTH<span class="op">-</span><span class="dv">1</span>)<span class="op">/</span>TILE_WIDTH)<span class="op">;</span></span>
<span id="cb3-196"><a href="#cb3-196" aria-hidden="true" tabindex="-1"></a>    outputKernel<span class="op">&lt;&lt;&lt;</span>gridOut, block<span class="op">&gt;&gt;&gt;</span>(dSoftmax, dV, dOut)<span class="op">;</span></span>
<span id="cb3-197"><a href="#cb3-197" aria-hidden="true" tabindex="-1"></a>    cudaDeviceSynchronize()<span class="op">;</span></span>
<span id="cb3-198"><a href="#cb3-198" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-199"><a href="#cb3-199" aria-hidden="true" tabindex="-1"></a>    <span class="op">//</span> Copy back</span>
<span id="cb3-200"><a href="#cb3-200" aria-hidden="true" tabindex="-1"></a>    cudaMemcpy(hOut, dOut, qSize, cudaMemcpyDeviceToHost)<span class="op">;</span></span>
<span id="cb3-201"><a href="#cb3-201" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-202"><a href="#cb3-202" aria-hidden="true" tabindex="-1"></a>    printf(<span class="st">"</span><span class="ch">\n</span><span class="st">Attention Output:</span><span class="ch">\n</span><span class="st">"</span>)<span class="op">;</span></span>
<span id="cb3-203"><a href="#cb3-203" aria-hidden="true" tabindex="-1"></a>    printMatrix(hOut, NUM_SAMPLES, FEATURE_DIMENSION)<span class="op">;</span></span>
<span id="cb3-204"><a href="#cb3-204" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-205"><a href="#cb3-205" aria-hidden="true" tabindex="-1"></a>    <span class="op">//</span> Cleanup</span>
<span id="cb3-206"><a href="#cb3-206" aria-hidden="true" tabindex="-1"></a>    free(hQ)<span class="op">;</span> free(hK)<span class="op">;</span> free(hV)<span class="op">;</span> free(hKT)<span class="op">;</span></span>
<span id="cb3-207"><a href="#cb3-207" aria-hidden="true" tabindex="-1"></a>    free(hScore)<span class="op">;</span> free(hSoftmax)<span class="op">;</span> free(hOut)<span class="op">;</span></span>
<span id="cb3-208"><a href="#cb3-208" aria-hidden="true" tabindex="-1"></a>    cudaFree(dQ)<span class="op">;</span> cudaFree(dKT)<span class="op">;</span> cudaFree(dV)<span class="op">;</span></span>
<span id="cb3-209"><a href="#cb3-209" aria-hidden="true" tabindex="-1"></a>    cudaFree(dScore)<span class="op">;</span> cudaFree(dSoftmax)<span class="op">;</span> cudaFree(dOut)<span class="op">;</span></span>
<span id="cb3-210"><a href="#cb3-210" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-211"><a href="#cb3-211" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="dv">0</span><span class="op">;</span></span>
<span id="cb3-212"><a href="#cb3-212" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-error">
<div class="ansi-escaped-output">
<pre><span class="ansi-cyan-fg">  Cell </span><span class="ansi-green-fg">In[3], line 35</span>
<span class="ansi-red-fg">    float acc = 0.0f;</span>
                  ^
<span class="ansi-red-fg">SyntaxError</span><span class="ansi-red-fg">:</span> invalid decimal literal
</pre>
</div>
</div>
</div>
</section>
<section id="flash-attention-in-cuda" class="level2">
<h2 class="anchored" data-anchor-id="flash-attention-in-cuda">Flash attention in CUDA</h2>
<p>Flash Attention is a specialized, fused‐kernel approach that was introduced to compute softmax(𝑄𝐾⊤)𝑉 in a single (or a small number of) GPU kernels, without ever storing the full 𝑁×𝑁 attention matrix in DRAM. Instead, it computes partial dot products and partial softmaxes in registers or shared memory, blocking in both the 𝑁 (sequence length) and 𝑑 (feature) dimensions. This greatly reduces memory bandwidth and peak memory usage, making it possible to handle very large sequences on a single GPU.</p>
<div id="734dd63d-ac97-4158-a1c4-5d9d0f8fe91f" class="cell">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co">#include &lt;stdio.h&gt;</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="co">#include &lt;cuda_runtime.h&gt;</span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="co">#include &lt;math.h&gt;</span></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="co">#include &lt;stdlib.h&gt;</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a><span class="co">#define NUM_SAMPLES 5</span></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a><span class="co">#define FEATURE_DIMENSION 6</span></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>void printMatrix(<span class="bu">float</span> <span class="op">*</span>matrix, <span class="bu">int</span> row, <span class="bu">int</span> col) {</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> (<span class="bu">int</span> r <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> r <span class="op">&lt;</span> row<span class="op">;</span> r<span class="op">++</span>) {</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> (<span class="bu">int</span> c <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> c <span class="op">&lt;</span> col<span class="op">;</span> c<span class="op">++</span>) {</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>            printf(<span class="st">"</span><span class="sc">%.3f</span><span class="st"> "</span>, matrix[r <span class="op">*</span> col <span class="op">+</span> c])<span class="op">;</span></span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>        }</span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>        printf(<span class="st">"</span><span class="ch">\n</span><span class="st">"</span>)<span class="op">;</span></span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a><span class="op">//</span> Kernel: Attention Score (x <span class="op">=</span> QK<span class="op">^</span>T)</span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a>__global__ void attention_score_kernel(</span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a>    <span class="bu">float</span> <span class="op">*</span>Q, <span class="bu">float</span> <span class="op">*</span>K, <span class="bu">float</span> <span class="op">*</span>x</span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a>) {</span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a>    <span class="bu">int</span> row <span class="op">=</span> blockIdx.y <span class="op">*</span> blockDim.y <span class="op">+</span> threadIdx.y<span class="op">;</span></span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a>    <span class="bu">int</span> col <span class="op">=</span> blockIdx.x <span class="op">*</span> blockDim.x <span class="op">+</span> threadIdx.x<span class="op">;</span></span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> (row <span class="op">&lt;</span> NUM_SAMPLES <span class="op">&amp;&amp;</span> col <span class="op">&lt;</span> NUM_SAMPLES) {</span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a>        <span class="bu">float</span> <span class="bu">sum</span> <span class="op">=</span> <span class="fl">0.0</span><span class="er">f</span><span class="op">;</span></span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> (<span class="bu">int</span> i <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> i <span class="op">&lt;</span> FEATURE_DIMENSION<span class="op">;</span> i<span class="op">++</span>) {</span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a>            <span class="bu">sum</span> <span class="op">+=</span> Q[row <span class="op">*</span> FEATURE_DIMENSION <span class="op">+</span> i] <span class="op">*</span> K[col <span class="op">*</span> FEATURE_DIMENSION <span class="op">+</span> i]<span class="op">;</span></span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a>        }</span>
<span id="cb4-30"><a href="#cb4-30" aria-hidden="true" tabindex="-1"></a>        x[row <span class="op">*</span> NUM_SAMPLES <span class="op">+</span> col] <span class="op">=</span> <span class="bu">sum</span><span class="op">;</span></span>
<span id="cb4-31"><a href="#cb4-31" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb4-32"><a href="#cb4-32" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb4-33"><a href="#cb4-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-34"><a href="#cb4-34" aria-hidden="true" tabindex="-1"></a><span class="op">//</span> Kernel: Flash Attention</span>
<span id="cb4-35"><a href="#cb4-35" aria-hidden="true" tabindex="-1"></a>__global__ void flash_attention_kernel(</span>
<span id="cb4-36"><a href="#cb4-36" aria-hidden="true" tabindex="-1"></a>    <span class="bu">float</span> <span class="op">*</span>x, <span class="bu">float</span> <span class="op">*</span>V, <span class="bu">float</span> <span class="op">*</span>O</span>
<span id="cb4-37"><a href="#cb4-37" aria-hidden="true" tabindex="-1"></a>) {</span>
<span id="cb4-38"><a href="#cb4-38" aria-hidden="true" tabindex="-1"></a>    <span class="bu">int</span> row <span class="op">=</span> blockIdx.y <span class="op">*</span> blockDim.y <span class="op">+</span> threadIdx.y<span class="op">;</span></span>
<span id="cb4-39"><a href="#cb4-39" aria-hidden="true" tabindex="-1"></a>    <span class="bu">int</span> col <span class="op">=</span> blockIdx.x <span class="op">*</span> blockDim.x <span class="op">+</span> threadIdx.x<span class="op">;</span></span>
<span id="cb4-40"><a href="#cb4-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-41"><a href="#cb4-41" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> (row <span class="op">&lt;</span> NUM_SAMPLES <span class="op">&amp;&amp;</span> col <span class="op">&lt;</span> FEATURE_DIMENSION) {</span>
<span id="cb4-42"><a href="#cb4-42" aria-hidden="true" tabindex="-1"></a>        <span class="bu">float</span> m <span class="op">=</span> <span class="op">-</span>INFINITY<span class="op">;</span></span>
<span id="cb4-43"><a href="#cb4-43" aria-hidden="true" tabindex="-1"></a>        <span class="bu">float</span> d <span class="op">=</span> <span class="fl">0.0</span><span class="er">f</span><span class="op">;</span></span>
<span id="cb4-44"><a href="#cb4-44" aria-hidden="true" tabindex="-1"></a>        <span class="bu">float</span> o <span class="op">=</span> <span class="fl">0.0</span><span class="er">f</span><span class="op">;</span></span>
<span id="cb4-45"><a href="#cb4-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-46"><a href="#cb4-46" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> (<span class="bu">int</span> i <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> i <span class="op">&lt;</span> NUM_SAMPLES<span class="op">;</span> i<span class="op">++</span>){</span>
<span id="cb4-47"><a href="#cb4-47" aria-hidden="true" tabindex="-1"></a>            <span class="bu">float</span> x_val <span class="op">=</span> x[row <span class="op">*</span> NUM_SAMPLES <span class="op">+</span> i]<span class="op">;</span></span>
<span id="cb4-48"><a href="#cb4-48" aria-hidden="true" tabindex="-1"></a>            <span class="bu">float</span> m_prev <span class="op">=</span> m<span class="op">;</span></span>
<span id="cb4-49"><a href="#cb4-49" aria-hidden="true" tabindex="-1"></a>            <span class="bu">float</span> d_prev <span class="op">=</span> d<span class="op">;</span></span>
<span id="cb4-50"><a href="#cb4-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-51"><a href="#cb4-51" aria-hidden="true" tabindex="-1"></a>            <span class="op">//</span> Compute running <span class="bu">max</span> <span class="kw">and</span> denominator</span>
<span id="cb4-52"><a href="#cb4-52" aria-hidden="true" tabindex="-1"></a>            m <span class="op">=</span> fmaxf(m_prev, x_val)<span class="op">;</span></span>
<span id="cb4-53"><a href="#cb4-53" aria-hidden="true" tabindex="-1"></a>            d <span class="op">=</span> (d_prev <span class="op">*</span> expf(m_prev <span class="op">-</span> m)) <span class="op">+</span> expf(x_val <span class="op">-</span> m)<span class="op">;</span></span>
<span id="cb4-54"><a href="#cb4-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-55"><a href="#cb4-55" aria-hidden="true" tabindex="-1"></a>            <span class="op">//</span> Compute output</span>
<span id="cb4-56"><a href="#cb4-56" aria-hidden="true" tabindex="-1"></a>            <span class="bu">float</span> v_val <span class="op">=</span> V[i <span class="op">*</span> FEATURE_DIMENSION <span class="op">+</span> col]<span class="op">;</span></span>
<span id="cb4-57"><a href="#cb4-57" aria-hidden="true" tabindex="-1"></a>            o <span class="op">=</span> o <span class="op">*</span> ((d_prev <span class="op">*</span> expf(m_prev <span class="op">-</span> m)) <span class="op">/</span> d) <span class="op">+</span> (expf(x_val<span class="op">-</span> m) <span class="op">/</span> d) <span class="op">*</span> v_val<span class="op">;</span></span>
<span id="cb4-58"><a href="#cb4-58" aria-hidden="true" tabindex="-1"></a>        }</span>
<span id="cb4-59"><a href="#cb4-59" aria-hidden="true" tabindex="-1"></a>        O[row <span class="op">*</span> FEATURE_DIMENSION <span class="op">+</span> col] <span class="op">=</span> o<span class="op">;</span></span>
<span id="cb4-60"><a href="#cb4-60" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb4-61"><a href="#cb4-61" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb4-62"><a href="#cb4-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-63"><a href="#cb4-63" aria-hidden="true" tabindex="-1"></a>void computeFlashAttention(</span>
<span id="cb4-64"><a href="#cb4-64" aria-hidden="true" tabindex="-1"></a>    <span class="bu">float</span> <span class="op">*</span>Q, <span class="bu">float</span> <span class="op">*</span>K, <span class="bu">float</span> <span class="op">*</span>V, <span class="bu">float</span> <span class="op">*</span>O</span>
<span id="cb4-65"><a href="#cb4-65" aria-hidden="true" tabindex="-1"></a>) {</span>
<span id="cb4-66"><a href="#cb4-66" aria-hidden="true" tabindex="-1"></a>    <span class="bu">float</span> <span class="op">*</span>d_Q, <span class="op">*</span>d_K, <span class="op">*</span>d_V, <span class="op">*</span>d_x, <span class="op">*</span>d_O<span class="op">;</span></span>
<span id="cb4-67"><a href="#cb4-67" aria-hidden="true" tabindex="-1"></a>    size_t size_1 <span class="op">=</span> NUM_SAMPLES <span class="op">*</span> FEATURE_DIMENSION <span class="op">*</span> sizeof(<span class="bu">float</span>)<span class="op">;</span></span>
<span id="cb4-68"><a href="#cb4-68" aria-hidden="true" tabindex="-1"></a>    size_t size_2 <span class="op">=</span> NUM_SAMPLES <span class="op">*</span> NUM_SAMPLES <span class="op">*</span> sizeof(<span class="bu">float</span>)<span class="op">;</span></span>
<span id="cb4-69"><a href="#cb4-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-70"><a href="#cb4-70" aria-hidden="true" tabindex="-1"></a>    <span class="op">//</span> Allocate device memory</span>
<span id="cb4-71"><a href="#cb4-71" aria-hidden="true" tabindex="-1"></a>    cudaMalloc((void<span class="op">**</span>)<span class="op">&amp;</span>d_Q, size_1)<span class="op">;</span></span>
<span id="cb4-72"><a href="#cb4-72" aria-hidden="true" tabindex="-1"></a>    cudaMalloc((void<span class="op">**</span>)<span class="op">&amp;</span>d_K, size_1)<span class="op">;</span></span>
<span id="cb4-73"><a href="#cb4-73" aria-hidden="true" tabindex="-1"></a>    cudaMalloc((void<span class="op">**</span>)<span class="op">&amp;</span>d_V, size_1)<span class="op">;</span></span>
<span id="cb4-74"><a href="#cb4-74" aria-hidden="true" tabindex="-1"></a>    cudaMalloc((void<span class="op">**</span>)<span class="op">&amp;</span>d_x, size_2)<span class="op">;</span></span>
<span id="cb4-75"><a href="#cb4-75" aria-hidden="true" tabindex="-1"></a>    cudaMalloc((void<span class="op">**</span>)<span class="op">&amp;</span>d_O, size_1)<span class="op">;</span></span>
<span id="cb4-76"><a href="#cb4-76" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-77"><a href="#cb4-77" aria-hidden="true" tabindex="-1"></a>    <span class="op">//</span> Copy data <span class="im">from</span> host to device</span>
<span id="cb4-78"><a href="#cb4-78" aria-hidden="true" tabindex="-1"></a>    cudaMemcpy(d_Q, Q, size_1, cudaMemcpyHostToDevice)<span class="op">;</span></span>
<span id="cb4-79"><a href="#cb4-79" aria-hidden="true" tabindex="-1"></a>    cudaMemcpy(d_K, K, size_1, cudaMemcpyHostToDevice)<span class="op">;</span></span>
<span id="cb4-80"><a href="#cb4-80" aria-hidden="true" tabindex="-1"></a>    cudaMemcpy(d_V, V, size_1, cudaMemcpyHostToDevice)<span class="op">;</span></span>
<span id="cb4-81"><a href="#cb4-81" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-82"><a href="#cb4-82" aria-hidden="true" tabindex="-1"></a>    <span class="op">//</span> Kernel launch <span class="cf">for</span> attention score</span>
<span id="cb4-83"><a href="#cb4-83" aria-hidden="true" tabindex="-1"></a>    dim3 blockDim(<span class="dv">16</span>, <span class="dv">16</span>, <span class="dv">1</span>)<span class="op">;</span></span>
<span id="cb4-84"><a href="#cb4-84" aria-hidden="true" tabindex="-1"></a>    dim3 gridDim((NUM_SAMPLES <span class="op">+</span> blockDim.x <span class="op">-</span> <span class="dv">1</span>)<span class="op">/</span>blockDim.x, (NUM_SAMPLES <span class="op">+</span> blockDim.y <span class="op">-</span> <span class="dv">1</span>)<span class="op">/</span>blockDim.y, <span class="dv">1</span>)<span class="op">;</span></span>
<span id="cb4-85"><a href="#cb4-85" aria-hidden="true" tabindex="-1"></a>    attention_score_kernel<span class="op">&lt;&lt;&lt;</span>gridDim, blockDim<span class="op">&gt;&gt;&gt;</span>(d_Q, d_K, d_x)<span class="op">;</span></span>
<span id="cb4-86"><a href="#cb4-86" aria-hidden="true" tabindex="-1"></a>    cudaDeviceSynchronize()<span class="op">;</span></span>
<span id="cb4-87"><a href="#cb4-87" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-88"><a href="#cb4-88" aria-hidden="true" tabindex="-1"></a>    <span class="op">//</span> Kernel launch <span class="cf">for</span> flash attention</span>
<span id="cb4-89"><a href="#cb4-89" aria-hidden="true" tabindex="-1"></a>    dim3 blockDim2(<span class="dv">16</span>, <span class="dv">16</span>, <span class="dv">1</span>)<span class="op">;</span></span>
<span id="cb4-90"><a href="#cb4-90" aria-hidden="true" tabindex="-1"></a>    dim3 gridDim2((NUM_SAMPLES <span class="op">+</span> blockDim2.x <span class="op">-</span> <span class="dv">1</span>)<span class="op">/</span>blockDim2.x, (NUM_SAMPLES <span class="op">+</span> blockDim2.y <span class="op">-</span> <span class="dv">1</span>)<span class="op">/</span>blockDim2.y, <span class="dv">1</span>)<span class="op">;</span></span>
<span id="cb4-91"><a href="#cb4-91" aria-hidden="true" tabindex="-1"></a>    flash_attention_kernel<span class="op">&lt;&lt;&lt;</span>gridDim2, blockDim2<span class="op">&gt;&gt;&gt;</span>(d_x, d_V, d_O)<span class="op">;</span></span>
<span id="cb4-92"><a href="#cb4-92" aria-hidden="true" tabindex="-1"></a>    cudaDeviceSynchronize()<span class="op">;</span></span>
<span id="cb4-93"><a href="#cb4-93" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-94"><a href="#cb4-94" aria-hidden="true" tabindex="-1"></a>    <span class="op">//</span> Copy Output <span class="im">from</span> device to host</span>
<span id="cb4-95"><a href="#cb4-95" aria-hidden="true" tabindex="-1"></a>    cudaMemcpy(O, d_O, size_1, cudaMemcpyDeviceToHost)<span class="op">;</span></span>
<span id="cb4-96"><a href="#cb4-96" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-97"><a href="#cb4-97" aria-hidden="true" tabindex="-1"></a>    <span class="op">//</span> Free device memory</span>
<span id="cb4-98"><a href="#cb4-98" aria-hidden="true" tabindex="-1"></a>    cudaFree(d_Q)<span class="op">;</span></span>
<span id="cb4-99"><a href="#cb4-99" aria-hidden="true" tabindex="-1"></a>    cudaFree(d_K)<span class="op">;</span></span>
<span id="cb4-100"><a href="#cb4-100" aria-hidden="true" tabindex="-1"></a>    cudaFree(d_V)<span class="op">;</span></span>
<span id="cb4-101"><a href="#cb4-101" aria-hidden="true" tabindex="-1"></a>    cudaFree(d_x)<span class="op">;</span></span>
<span id="cb4-102"><a href="#cb4-102" aria-hidden="true" tabindex="-1"></a>    cudaFree(d_O)<span class="op">;</span></span>
<span id="cb4-103"><a href="#cb4-103" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb4-104"><a href="#cb4-104" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-105"><a href="#cb4-105" aria-hidden="true" tabindex="-1"></a><span class="bu">int</span> main() {</span>
<span id="cb4-106"><a href="#cb4-106" aria-hidden="true" tabindex="-1"></a>    <span class="bu">float</span> size <span class="op">=</span> FEATURE_DIMENSION <span class="op">*</span> NUM_SAMPLES <span class="op">*</span> sizeof(<span class="bu">float</span>)<span class="op">;</span></span>
<span id="cb4-107"><a href="#cb4-107" aria-hidden="true" tabindex="-1"></a>    <span class="bu">float</span> <span class="op">*</span>Q <span class="op">=</span> (<span class="bu">float</span> <span class="op">*</span>)malloc(size)<span class="op">;</span></span>
<span id="cb4-108"><a href="#cb4-108" aria-hidden="true" tabindex="-1"></a>    <span class="bu">float</span> <span class="op">*</span>K <span class="op">=</span> (<span class="bu">float</span> <span class="op">*</span>)malloc(size)<span class="op">;</span></span>
<span id="cb4-109"><a href="#cb4-109" aria-hidden="true" tabindex="-1"></a>    <span class="bu">float</span> <span class="op">*</span>V <span class="op">=</span> (<span class="bu">float</span> <span class="op">*</span>)malloc(size)<span class="op">;</span></span>
<span id="cb4-110"><a href="#cb4-110" aria-hidden="true" tabindex="-1"></a>    <span class="bu">float</span> <span class="op">*</span>O <span class="op">=</span> (<span class="bu">float</span> <span class="op">*</span>)malloc(size)<span class="op">;</span></span>
<span id="cb4-111"><a href="#cb4-111" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-112"><a href="#cb4-112" aria-hidden="true" tabindex="-1"></a>    <span class="op">//</span> Initialize matrices</span>
<span id="cb4-113"><a href="#cb4-113" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> (<span class="bu">int</span> i <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> i <span class="op">&lt;</span> NUM_SAMPLES <span class="op">*</span> FEATURE_DIMENSION<span class="op">;</span> i<span class="op">++</span>) {</span>
<span id="cb4-114"><a href="#cb4-114" aria-hidden="true" tabindex="-1"></a>        Q[i] <span class="op">=</span> rand() <span class="op">%</span> <span class="dv">50</span><span class="op">;</span></span>
<span id="cb4-115"><a href="#cb4-115" aria-hidden="true" tabindex="-1"></a>        K[i] <span class="op">=</span> rand() <span class="op">%</span> <span class="dv">50</span><span class="op">;</span></span>
<span id="cb4-116"><a href="#cb4-116" aria-hidden="true" tabindex="-1"></a>        V[i] <span class="op">=</span> rand() <span class="op">%</span> <span class="dv">50</span><span class="op">;</span></span>
<span id="cb4-117"><a href="#cb4-117" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb4-118"><a href="#cb4-118" aria-hidden="true" tabindex="-1"></a>    printf(<span class="st">"</span><span class="ch">\n</span><span class="st">Query:</span><span class="ch">\n</span><span class="st">"</span>)<span class="op">;</span> printMatrix(Q, NUM_SAMPLES, FEATURE_DIMENSION)<span class="op">;</span></span>
<span id="cb4-119"><a href="#cb4-119" aria-hidden="true" tabindex="-1"></a>    printf(<span class="st">"</span><span class="ch">\n</span><span class="st">Key:</span><span class="ch">\n</span><span class="st">"</span>)<span class="op">;</span>   printMatrix(K, NUM_SAMPLES, FEATURE_DIMENSION)<span class="op">;</span></span>
<span id="cb4-120"><a href="#cb4-120" aria-hidden="true" tabindex="-1"></a>    printf(<span class="st">"</span><span class="ch">\n</span><span class="st">Value:</span><span class="ch">\n</span><span class="st">"</span>)<span class="op">;</span> printMatrix(V, NUM_SAMPLES, FEATURE_DIMENSION)<span class="op">;</span></span>
<span id="cb4-121"><a href="#cb4-121" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-122"><a href="#cb4-122" aria-hidden="true" tabindex="-1"></a>    <span class="op">//</span> Compute Flash Attention</span>
<span id="cb4-123"><a href="#cb4-123" aria-hidden="true" tabindex="-1"></a>    computeFlashAttention(Q, K, V, O)<span class="op">;</span></span>
<span id="cb4-124"><a href="#cb4-124" aria-hidden="true" tabindex="-1"></a>    printf(<span class="st">"</span><span class="ch">\n</span><span class="st">Output:</span><span class="ch">\n</span><span class="st">"</span>)<span class="op">;</span> printMatrix(O, NUM_SAMPLES, FEATURE_DIMENSION)<span class="op">;</span></span>
<span id="cb4-125"><a href="#cb4-125" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-126"><a href="#cb4-126" aria-hidden="true" tabindex="-1"></a>    <span class="op">//</span> Free host memory</span>
<span id="cb4-127"><a href="#cb4-127" aria-hidden="true" tabindex="-1"></a>    free(Q)<span class="op">;</span></span>
<span id="cb4-128"><a href="#cb4-128" aria-hidden="true" tabindex="-1"></a>    free(K)<span class="op">;</span></span>
<span id="cb4-129"><a href="#cb4-129" aria-hidden="true" tabindex="-1"></a>    free(V)<span class="op">;</span></span>
<span id="cb4-130"><a href="#cb4-130" aria-hidden="true" tabindex="-1"></a>    free(O)<span class="op">;</span></span>
<span id="cb4-131"><a href="#cb4-131" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-132"><a href="#cb4-132" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="dv">0</span><span class="op">;</span></span>
<span id="cb4-133"><a href="#cb4-133" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<section id="explanation-of-key-steps" class="level4">
<h4 class="anchored" data-anchor-id="explanation-of-key-steps"><strong>Explanation of Key Steps</strong></h4>
<ul>
<li><strong>Single-Kernel Fusion:</strong> Unlike the naive version (which had 3 separate kernels: dot-product, softmax, matmul), Flash Attention does everything in one kernel launch.</li>
<li><strong>Shared Memory Usage for K and V:</strong> We load the entire 𝐾 and 𝑉 into shared memory once.</li>
<li><strong>Computing row_max and row_sum in a Streaming Fashion:</strong> We do a two-pass approach (but within the same kernel):
<ul>
<li>Pass 1: scan over all keys to find row_max.</li>
<li>Pass 2: scan again over all keys to accumulate row_sum = ∑ exp((QK)/√d – row_max). This two-pass trick avoids having to store all raw scores. We only keep track of two scalars per query.</li>
</ul></li>
<li><strong>Computing Final Weighted Sum Over V:</strong> Once we know A𝑖,𝑗 we multiply by 𝑉𝑗. Since 𝑉𝑗 is already in shared memory (for that entire tile of keys), we can do this accumulation without accessing DRAM for every (𝑖,𝑗).</li>
</ul>
</section>
<section id="why-this-is-flash-fast-low-memory" class="level4">
<h4 class="anchored" data-anchor-id="why-this-is-flash-fast-low-memory"><strong>Why This Is “Flash” (Fast + Low Memory):</strong></h4>
<p>Fused Kernel means we only launch one CUDA kernel—no separate writes/reads of a big 𝑁×𝑁 attention-matrix. The partial dot products and exponentials are handled “on the fly” in registers or shared memory.</p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>