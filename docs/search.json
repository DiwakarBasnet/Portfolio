[
  {
    "objectID": "blogs/attention/index.html",
    "href": "blogs/attention/index.html",
    "title": "Self-Attention",
    "section": "",
    "text": "Since its introduction via the original transformer paper “Attention is all you need”, self-attention has become the corner stone of many state-of-the-art deep learning models, specially in the field of NLP. Self-attention mechanisms enable models to weigh different parts of input data differently, focusing on the most relevant information while performing a task. This mimics the human ability to selectively pay attention to certain aspects of our surroundings while filtering out distractions. Attention mechanisms have been instrumental in improving the performance of various AI models, particularly in sequence-to-sequence tasks.\nIn a transformer architecture, “query”, “key” and “value” are the fundamental components used for calculating the self-attention. In simple terms, suppose we have a book related to animals; query represents the question that one might have such as “what is the largest mammal on earth?”. Similarly, key represents the index or table of content in the book and value represents the actual answer that we obtain from the book, in this case “blue whale”. In technical terms;. Query: A query is a matrix that represents the current token of request in the input sequence. Each word in the sequence has an associated query vector. 2. Key: A key is also a matrix that represents the content or identity of each token. 3. Value: A value is the actual information of each token that can be passed along.\nFor each token in the input sequence; the self-attention model computes query vector from it, computes key and value vectors from every token in the sequence, calculates attention score by taking dot product of query and each key, applies softmax and finally computes the output as the weighted sum of the value vectors."
  },
  {
    "objectID": "blogs/attention/index.html#mathematics-of-self-attention",
    "href": "blogs/attention/index.html#mathematics-of-self-attention",
    "title": "Self-Attention",
    "section": "Mathematics of self-attention",
    "text": "Mathematics of self-attention\n\nLearnable Projections\nThree trainable weight matrices, transform ( X ) into queries ( Q ), keys ( K ), and values ( V ):\n\\[Q = X \\cdot W^Q, \\quad W^Q \\in \\mathbb{R}^{d \\times d_k}\\]\n\\[K = X \\cdot W^K, \\quad W^K \\in \\mathbb{R}^{d \\times d_k}\\]\n\\[V = X \\cdot W^V, \\quad W^V \\in \\mathbb{R}^{d \\times d_v}\\]\nWhere: - \\(( d_k )\\): dimension of queries/keys - \\(( d_v )\\): dimension of values\n\n\nScaled Dot-Product Attention\n\nStep 1: Compute Attention Scores\nCompute all pairwise “compatibility” scores between queries and keys \\[scores_{i,j} = Q_i \\cdot K_j^T \\in \\mathbb{R}^{n \\times n}\\]\n\n\nStep 2: Scale Scores\nBecause the magnitude of dot products grows with dimension \\({d_k}\\), we divide by \\({\\sqrt{d_k}}\\): \\[scaled_scores_{i,j} = \\frac{scores_{i,j}}{\\sqrt{d_k}}\\]\n\n\nStep 3: Softmax Normalization\nFor each query i, we want to convert its scores scaled_scores into a probability distribution over the N keys: \\[A_{i,j} = softmax(scaled_scores_{i,j}), \\quad A_{i,j} = \\frac{e^{s_{i,j}}}{\\sum_{k=1}^n e^{s_{i,k}}}\\]\n\n\nStep 4: Weighted Sum of Values\n\\[Output (O_i) = A_{i,j} \\cdot V_j \\in \\mathbb{R}^{n \\times d_v}\\]"
  },
  {
    "objectID": "blogs/attention/index.html#self-attention-implementation-in-cpu",
    "href": "blogs/attention/index.html#self-attention-implementation-in-cpu",
    "title": "Self-Attention",
    "section": "Self-attention implementation in CPU",
    "text": "Self-attention implementation in CPU\nBelow is a step-by-step code walkthrough of C implementation that runs entirely on CPU.\nHigh-level structure: * Allocate and initialize input query/key/value matrices (query, key, value). * Compute the attention scores attentionScores = Q × Kᵀ (with loops). * Apply scaling + row-wise softmax to produce softmaxedScores. * Compute output = softmaxedScores × V.\n\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;math.h&gt;\n\n#define NUM_SAMPLES 2\n#define FEATURE_DIMENSION 3\n\nvoid printMatrix(float *matrix, int row, int col) {\n    for (int i = 0; i &lt; row; i++) {\n        for (int j = 0; j &lt; col; j++) {\n            printf(\"%f \", matrix[i * col + j]);\n        }\n        printf(\"\\n\");\n    }\n}\n\n// CPU Implementation of Attention\nvoid transposeMatrix(float *in_matrix, float *out_matrix, int row, int col) {\n    for (int i = 0; i &lt; row; i++) {\n        for (int j = 0; j &lt; col; j++) {\n            out_matrix[j * row + i] = in_matrix[i * col + j];\n        }\n    }\n}\n\nvoid computeAttentionCPU(float *query, float *key, float *value,\n        float *attentionScores, float *output) {\n    float *transposeKey = (float*)malloc(FEATURE_DIMENSION * NUM_SAMPLES * sizeof(float));\n    transposeMatrix(key, transposeKey, NUM_SAMPLES, FEATURE_DIMENSION);\n\n    float scalingFactor = 1.0f / sqrt((float)FEATURE_DIMENSION);\n\n    // Compute attention scores\n    for (int i = 0; i &lt; NUM_SAMPLES; i++) {\n        for (int j = 0; j &lt; NUM_SAMPLES; j++) {\n            for (int k = 0; k &lt; FEATURE_DIMENSION; k++) {\n                attentionScores[i * NUM_SAMPLES + j] += query[i * FEATURE_DIMENSION + k] * transposeKey[k * NUM_SAMPLES + j];\n            }\n            attentionScores[i * NUM_SAMPLES + j] *= scalingFactor;\n        }\n    }\n\n    // Softmax row-wise\n    for (int row = 0; row &lt; NUM_SAMPLES; row++) {\n        float maxScore = attentionScores[row * NUM_SAMPLES];\n        for (int col = 1; col &lt; NUM_SAMPLES; col++) {\n            if (attentionScores[row * NUM_SAMPLES + col] &gt; maxScore) {\n                maxScore = attentionScores[row * NUM_SAMPLES + col];\n            }\n        }\n        float sumExp = 0.0f;\n        for (int col = 0; col &lt; NUM_SAMPLES; col++) {\n            attentionScores[row * NUM_SAMPLES + col] = expf(attentionScores[row * NUM_SAMPLES + col] - maxScore);\n            sumExp += attentionScores[row * NUM_SAMPLES + col];\n        }\n        for (int col = 0; col &lt; NUM_SAMPLES; col++) {\n            attentionScores[row * NUM_SAMPLES + col] /= sumExp;\n        }\n    }\n\n    // Multiply by value matrix\n    for (int i = 0; i &lt; NUM_SAMPLES; i++) {\n        for (int j = 0; j &lt; FEATURE_DIMENSION; j++) {\n            for (int k = 0; k &lt; NUM_SAMPLES; k++) {\n                output[i * FEATURE_DIMENSION + j] += attentionScores[i * NUM_SAMPLES + k] * value[k * FEATURE_DIMENSION + j];\n            }\n        }\n    }\n\n    free(transposeKey);\n}\n\nint main() {\n    float query[NUM_SAMPLES * FEATURE_DIMENSION] = {\n        1.0f, 0.0f, -1.0f,\n        0.5f, 0.5f, 0.5f\n    };\n\n    float key[NUM_SAMPLES * FEATURE_DIMENSION] = {\n        1.0f, 2.0f, 3.0f,\n        4.0f, 5.0f, 6.0f\n    };\n    \n    float value[NUM_SAMPLES * FEATURE_DIMENSION] = {\n        1.0f, 1.0f, 1.0f,\n        2.0f, 2.0f, 2.0f\n    };\n\n    float* output = (float*)malloc(FEATURE_DIMENSION * NUM_SAMPLES * sizeof(float));\n    float* attentionScores = (float*)malloc(NUM_SAMPLES * NUM_SAMPLES * sizeof(float));\n    computeAttentionCPU(query, key, value, attentionScores, output);\n\n    printMatrix(output, NUM_SAMPLES, FEATURE_DIMENSION);\n\n    free(output);\n    free(attentionScores);\n\n    return 0;\n}"
  },
  {
    "objectID": "blogs/attention/index.html#naive-self-attention-implementation-in-cuda",
    "href": "blogs/attention/index.html#naive-self-attention-implementation-in-cuda",
    "title": "Self-Attention",
    "section": "Naive self-attention implementation in CUDA",
    "text": "Naive self-attention implementation in CUDA\nThis naive approach simply offloads the same steps to GPU, but without any fancy shared-memory tiling. Each thread computes one element of the matrix multiply or one element of the output. This is not memory- or compute-optimal, but it’s the easiest way to see how we map loops to kernels.\n\n#include &lt;stdio.h&gt;\n#include &lt;cuda_runtime.h&gt;\n#include &lt;math.h&gt;\n#include &lt;stdlib.h&gt;\n\n#define NUM_SAMPLES 5\n#define FEATURE_DIMENSION 6\n\nvoid printMatrix(float *matrix, int row, int col) {\n    for (int i = 0; i &lt; row; i++) {\n        for (int j = 0; j &lt; col; j++) {\n            printf(\"%.3f \", matrix[i * col + j]);\n        }\n        printf(\"\\n\");\n    }\n}\n\n// Kernel: Softmax\n__global__ void softmaxKernel(float *scoreMatrix, float *softmaxMatrix) {\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    if (row &lt; NUM_SAMPLES) {\n        float maxScore = -1e30f;\n        for (int col = 0; col &lt; NUM_SAMPLES; ++col) {\n            maxScore = fmaxf(maxScore, scoreMatrix[row * NUM_SAMPLES + col]);\n        }\n        float sumExp = 0.0f;\n        for (int col = 0; col &lt; NUM_SAMPLES; ++col) {\n            softmaxMatrix[row * NUM_SAMPLES + col] = \n                expf(scoreMatrix[row * NUM_SAMPLES + col] - maxScore);\n            sumExp += softmaxMatrix[row * NUM_SAMPLES + col];\n        }\n        for (int col = 0; col &lt; NUM_SAMPLES; ++col) {\n            softmaxMatrix[row * NUM_SAMPLES + col] /= sumExp;\n        }\n    }\n}\n\n// Kernel: QK^T\n__global__ void computeScoreKernel(float *queryMatrix, float *keyMatrix, float *scoreMatrix) {\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n    if (row &lt; NUM_SAMPLES && col &lt; NUM_SAMPLES) {\n        float score = 0.0f;\n        for (int d = 0; d &lt; FEATURE_DIMENSION; ++d) {\n            score += queryMatrix[row * FEATURE_DIMENSION + d] *\n                keyMatrix[col * FEATURE_DIMENSION + d];\n        }\n        scoreMatrix[row * NUM_SAMPLES + col] = score / sqrtf(static_cast&lt;float&gt;(FEATURE_DIMENSION));\n    }\n}\n\n// Kernel: Output = Softmax(QK^T) * V\n__global__ void computeOutputKernel(float * softmaxMatrix, float *valueMatrix, float *outputMatrix) {\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n    if (row &lt; NUM_SAMPLES && col &lt; FEATURE_DIMENSION) {\n        float result = 0.0f;\n        for (int k = 0; k &lt; NUM_SAMPLES; ++k) {\n            result += softmaxMatrix[row * NUM_SAMPLES + k] *\n                valueMatrix[k * FEATURE_DIMENSION + col];\n        }\n        outputMatrix[row * FEATURE_DIMENSION + col] = result;\n    }\n}\n\nvoid computeAttention(float *queryMatrix_h, float *keyMatrix_h, float *valueMatrix_h, float *attnMatrix_h) {\n    float size = NUM_SAMPLES * FEATURE_DIMENSION * sizeof(float);\n    float size_temp = NUM_SAMPLES * NUM_SAMPLES * sizeof(float);\n\n    float *queryMatrix, *keyMatrix, *valueMatrix, *attnMatrix, *scoreMatrix, *softmaxMatrix;\n\n    // Device memory allocation\n    cudaMalloc((void**)&queryMatrix, size);\n    cudaMalloc((void**)&keyMatrix, size);\n    cudaMalloc((void**)&valueMatrix, size);\n    cudaMalloc((void**)&attnMatrix, size);\n    cudaMalloc((void**)&scoreMatrix, size_temp);\n    cudaMalloc((void**)&softmaxMatrix, size_temp);\n\n    cudaMemcpy(queryMatrix, queryMatrix_h, size, cudaMemcpyHostToDevice);\n    cudaMemcpy(keyMatrix, keyMatrix_h, size, cudaMemcpyHostToDevice);\n    cudaMemcpy(valueMatrix, valueMatrix_h, size, cudaMemcpyHostToDevice);\n    \n    // Kernel initializations\n    dim3 blockDim(16, 16, 1);\n    dim3 gridDim((NUM_SAMPLES+blockDim.x-1)/blockDim.x, (NUM_SAMPLES+blockDim.y-1)/blockDim.y, 1);\n    computeScoreKernel&lt;&lt;&lt;gridDim, blockDim&gt;&gt;&gt;(queryMatrix, keyMatrix, scoreMatrix);\n    cudaDeviceSynchronize();\n\n    dim3 softmaxBlockDim(16, 16, 1);\n    dim3 softmaxGridDim((NUM_SAMPLES+softmaxBlockDim.x-1)/softmaxBlockDim.x, (NUM_SAMPLES+softmaxBlockDim.y-1)/softmaxBlockDim.y, 1);\n    softmaxKernel&lt;&lt;&lt;softmaxGridDim, softmaxBlockDim&gt;&gt;&gt;(scoreMatrix, softmaxMatrix);\n    cudaDeviceSynchronize();\n\n    dim3 outputBlockDim(16, 16, 1);\n    dim3 outputGridDim((NUM_SAMPLES+outputBlockDim.x-1)/outputBlockDim.x, (NUM_SAMPLES+outputBlockDim.y-1)/outputBlockDim.y, 1);\n    computeOutputKernel&lt;&lt;&lt;outputGridDim, outputBlockDim&gt;&gt;&gt;(softmaxMatrix, valueMatrix, attnMatrix);\n    cudaDeviceSynchronize();\n\n    // Copy output from device to host\n    cudaMemcpy(attnMatrix_h, attnMatrix, size, cudaMemcpyDeviceToHost);\n\n    cudaFree(queryMatrix);\n    cudaFree(keyMatrix);\n    cudaFree(valueMatrix);\n    cudaFree(attnMatrix);\n    cudaFree(scoreMatrix);\n    cudaFree(softmaxMatrix);\n}\n\nint main() {\n    int size = NUM_SAMPLES * FEATURE_DIMENSION * sizeof(float);\n\n    float *queryMatrix = (float *)malloc(size);\n    float *keyMatrix = (float *)malloc(size);\n    float *valueMatrix = (float *)malloc(size);\n    float *attnMatrix = (float *)malloc(size);\n\n    // Initialize matrix\n    for (int i = 0; i &lt; NUM_SAMPLES * FEATURE_DIMENSION; i++) {\n        queryMatrix[i] = (float)(rand() % 50);\n        keyMatrix[i] = (float)(rand() % 50);\n        valueMatrix[i] = (float)(rand() % 50);\n    }\n\n    printf(\"\\nQuery:\\n\");\n    printMatrix(queryMatrix, NUM_SAMPLES, FEATURE_DIMENSION);\n\n    printf(\"\\nKey:\\n\");\n    printMatrix(keyMatrix, NUM_SAMPLES, FEATURE_DIMENSION);\n\n    printf(\"\\nValue\\n\");\n    printMatrix(valueMatrix, NUM_SAMPLES, FEATURE_DIMENSION);\n\n    // Attention calculation\n    computeAttention(queryMatrix, keyMatrix, valueMatrix, attnMatrix);\n\n    // Print attention matrix\n    printf(\"\\nAttention matrix;\\:\\n\");\n    printMatrix(attnMatrix, NUM_SAMPLES, FEATURE_DIMENSION);\n\n    // Free memory\n    free(queryMatrix);\n    free(keyMatrix);\n    free(valueMatrix);\n    free(attnMatrix);\n\n    return 0;\n}\n\n\n  Cell In[2], line 22\n    float maxScore = -1e30f;\n                         ^\nSyntaxError: invalid decimal literal"
  },
  {
    "objectID": "blogs/attention/index.html#optimized-self-attention-in-cuda",
    "href": "blogs/attention/index.html#optimized-self-attention-in-cuda",
    "title": "Self-Attention",
    "section": "Optimized self-attention in CUDA",
    "text": "Optimized self-attention in CUDA\nThis approach uses tiled/shared-memory to speed up the 𝑄𝐾⊤ and the final softmax × V operations. By loading contiguous chunks (tiles) of the input matrices into shared memory, threads within a block can cooperatively reuse data, minimizing expensive global‐memory round trips.\n\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;math.h&gt;\n#include &lt;cuda_runtime.h&gt;\n\n#define NUM_SAMPLES 5\n#define FEATURE_DIMENSION 6\n#define TILE_WIDTH 16\n\n// Print utility\nvoid printMatrix(const float* matrix, int rows, int cols) {\n    for (int r = 0; r &lt; rows; ++r) {\n        for (int c = 0; c &lt; cols; ++c) {\n            printf(\"%.3f \", matrix[r * cols + c]);\n        }\n        printf(\"\\n\");\n    }\n}\n\n// Kernel: compute Q * K^T (scores)\n__global__ void scoreKernel(\n    const float* __restrict__ query,\n    const float* __restrict__ keyT,\n    float* __restrict__ score) {\n    __shared__ float sharedQ[TILE_WIDTH][TILE_WIDTH];\n    __shared__ float sharedK[TILE_WIDTH][TILE_WIDTH];\n\n    int tx = threadIdx.x;\n    int ty = threadIdx.y;\n    int bx = blockIdx.x;\n    int by = blockIdx.y;\n\n    int col = bx * TILE_WIDTH + tx;\n    int row = by * TILE_WIDTH + ty;\n    float acc = 0.0f;\n\n    int phases = (FEATURE_DIMENSION + TILE_WIDTH - 1) / TILE_WIDTH;\n    for (int p = 0; p &lt; phases; ++p) {\n        int qCol = p * TILE_WIDTH + tx;\n        int kRow = p * TILE_WIDTH + ty;\n\n        // Load Q tile\n        if (row &lt; NUM_SAMPLES && qCol &lt; FEATURE_DIMENSION)\n            sharedQ[ty][tx] = query[row * FEATURE_DIMENSION + qCol];\n        else\n            sharedQ[ty][tx] = 0.0f;\n        // Load K^T tile\n        if (col &lt; NUM_SAMPLES && kRow &lt; FEATURE_DIMENSION)\n            sharedK[ty][tx] = keyT[kRow * NUM_SAMPLES + col];\n        else\n            sharedK[ty][tx] = 0.0f;\n        __syncthreads();\n\n        // Dot-product\n        for (int i = 0; i &lt; TILE_WIDTH; ++i) {\n            acc += sharedQ[ty][i] * sharedK[i][tx];\n        }\n        __syncthreads();\n    }\n\n    if (row &lt; NUM_SAMPLES && col &lt; NUM_SAMPLES) {\n        score[row * NUM_SAMPLES + col] = acc / sqrtf((float)FEATURE_DIMENSION);\n    }\n}\n\n// Kernel: row-wise softmax\n__global__ void softmaxKernel(\n    const float* __restrict__ score,\n    float* __restrict__ softmax) {\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    if (row &lt; NUM_SAMPLES) {\n        float maxv = -1e30f;\n        for (int c = 0; c &lt; NUM_SAMPLES; ++c)\n            maxv = fmaxf(maxv, score[row * NUM_SAMPLES + c]);\n        float sum = 0.0f;\n        for (int c = 0; c &lt; NUM_SAMPLES; ++c) {\n            float e = expf(score[row * NUM_SAMPLES + c] - maxv);\n            softmax[row * NUM_SAMPLES + c] = e;\n            sum += e;\n        }\n        for (int c = 0; c &lt; NUM_SAMPLES; ++c)\n            softmax[row * NUM_SAMPLES + c] /= sum;\n    }\n}\n\n// Kernel: softmax * V\n__global__ void outputKernel(\n    const float* __restrict__ softmax,\n    const float* __restrict__ value,\n    float* __restrict__ output) {\n    __shared__ float sharedS[TILE_WIDTH][TILE_WIDTH];\n    __shared__ float sharedV[TILE_WIDTH][TILE_WIDTH];\n\n    int tx = threadIdx.x;\n    int ty = threadIdx.y;\n    int bx = blockIdx.x;\n    int by = blockIdx.y;\n\n    int col = bx * TILE_WIDTH + tx;\n    int row = by * TILE_WIDTH + ty;\n    float acc = 0.0f;\n\n    int phases = (NUM_SAMPLES + TILE_WIDTH - 1) / TILE_WIDTH;\n    for (int p = 0; p &lt; phases; ++p) {\n        int sCol = p * TILE_WIDTH + tx;\n        int vRow = p * TILE_WIDTH + ty;\n\n        // Load softmax tile\n        if (row &lt; NUM_SAMPLES && sCol &lt; NUM_SAMPLES)\n            sharedS[ty][tx] = softmax[row * NUM_SAMPLES + sCol];\n        else\n            sharedS[ty][tx] = 0.0f;\n        // Load V tile\n        if (vRow &lt; NUM_SAMPLES && col &lt; FEATURE_DIMENSION)\n            sharedV[ty][tx] = value[vRow * FEATURE_DIMENSION + col];\n        else\n            sharedV[ty][tx] = 0.0f;\n        __syncthreads();\n\n        // Dot-product\n        for (int i = 0; i &lt; TILE_WIDTH; ++i) {\n            acc += sharedS[ty][i] * sharedV[i][tx];\n        }\n        __syncthreads();\n    }\n\n    if (row &lt; NUM_SAMPLES && col &lt; FEATURE_DIMENSION) {\n        output[row * FEATURE_DIMENSION + col] = acc;\n    }\n}\n\n// Host helper: transpose key\nvoid transposeKey(const float* key, float* keyT) {\n    for (int r = 0; r &lt; NUM_SAMPLES; ++r)\n        for (int c = 0; c &lt; FEATURE_DIMENSION; ++c)\n            keyT[c * NUM_SAMPLES + r] = key[r * FEATURE_DIMENSION + c];\n}\n\nint main() {\n    size_t qSize = NUM_SAMPLES * FEATURE_DIMENSION * sizeof(float);\n    size_t kTSize = NUM_SAMPLES * FEATURE_DIMENSION * sizeof(float);\n    size_t sSize = NUM_SAMPLES * NUM_SAMPLES * sizeof(float);\n\n    // Host allocations\n    float *hQ = (float*)malloc(qSize);\n    float *hK = (float*)malloc(qSize);\n    float *hV = (float*)malloc(qSize);\n    float *hKT = (float*)malloc(kTSize);\n    float *hScore = (float*)malloc(sSize);\n    float *hSoftmax = (float*)malloc(sSize);\n    float *hOut = (float*)malloc(qSize);\n\n    // Random init\n    for (int i = 0; i &lt; NUM_SAMPLES * FEATURE_DIMENSION; ++i) {\n        hQ[i] = rand() % 50;\n        hK[i] = rand() % 50;\n        hV[i] = rand() % 50;\n    }\n\n    printf(\"\\nQuery:\\n\"); printMatrix(hQ, NUM_SAMPLES, FEATURE_DIMENSION);\n    printf(\"\\nKey:\\n\");   printMatrix(hK, NUM_SAMPLES, FEATURE_DIMENSION);\n    printf(\"\\nValue:\\n\"); printMatrix(hV, NUM_SAMPLES, FEATURE_DIMENSION);\n\n    // Transpose key on host\n    transposeKey(hK, hKT);\n\n    // Device allocations\n    float *dQ, *dKT, *dV, *dScore, *dSoftmax, *dOut;\n    cudaMalloc(&dQ, qSize);\n    cudaMalloc(&dKT, kTSize);\n    cudaMalloc(&dV, qSize);\n    cudaMalloc(&dScore, sSize);\n    cudaMalloc(&dSoftmax, sSize);\n    cudaMalloc(&dOut, qSize);\n\n    // Copy to device\n    cudaMemcpy(dQ, hQ, qSize, cudaMemcpyHostToDevice);\n    cudaMemcpy(dKT, hKT, kTSize, cudaMemcpyHostToDevice);\n    cudaMemcpy(dV, hV, qSize, cudaMemcpyHostToDevice);\n\n    // Launch score kernel\n    dim3 block(TILE_WIDTH, TILE_WIDTH);\n    dim3 gridScore((NUM_SAMPLES+TILE_WIDTH-1)/TILE_WIDTH,\n                   (NUM_SAMPLES+TILE_WIDTH-1)/TILE_WIDTH);\n    scoreKernel&lt;&lt;&lt;gridScore, block&gt;&gt;&gt;(dQ, dKT, dScore);\n    cudaDeviceSynchronize();\n\n    // Softmax kernel\n    dim3 gridSm((NUM_SAMPLES+TILE_WIDTH-1)/TILE_WIDTH, 1);\n    softmaxKernel&lt;&lt;&lt;gridSm, block&gt;&gt;&gt;(dScore, dSoftmax);\n    cudaDeviceSynchronize();\n\n    // Output kernel\n    dim3 gridOut((FEATURE_DIMENSION+TILE_WIDTH-1)/TILE_WIDTH,\n                 (NUM_SAMPLES+TILE_WIDTH-1)/TILE_WIDTH);\n    outputKernel&lt;&lt;&lt;gridOut, block&gt;&gt;&gt;(dSoftmax, dV, dOut);\n    cudaDeviceSynchronize();\n\n    // Copy back\n    cudaMemcpy(hOut, dOut, qSize, cudaMemcpyDeviceToHost);\n\n    printf(\"\\nAttention Output:\\n\");\n    printMatrix(hOut, NUM_SAMPLES, FEATURE_DIMENSION);\n\n    // Cleanup\n    free(hQ); free(hK); free(hV); free(hKT);\n    free(hScore); free(hSoftmax); free(hOut);\n    cudaFree(dQ); cudaFree(dKT); cudaFree(dV);\n    cudaFree(dScore); cudaFree(dSoftmax); cudaFree(dOut);\n\n    return 0;\n}\n\n\n  Cell In[3], line 35\n    float acc = 0.0f;\n                  ^\nSyntaxError: invalid decimal literal"
  },
  {
    "objectID": "blogs/attention/index.html#flash-attention-in-cuda",
    "href": "blogs/attention/index.html#flash-attention-in-cuda",
    "title": "Self-Attention",
    "section": "Flash attention in CUDA",
    "text": "Flash attention in CUDA\nFlash Attention is a specialized, fused‐kernel approach that was introduced to compute softmax(𝑄𝐾⊤)𝑉 in a single (or a small number of) GPU kernels, without ever storing the full 𝑁×𝑁 attention matrix in DRAM. Instead, it computes partial dot products and partial softmaxes in registers or shared memory, blocking in both the 𝑁 (sequence length) and 𝑑 (feature) dimensions. This greatly reduces memory bandwidth and peak memory usage, making it possible to handle very large sequences on a single GPU.\n\n#include &lt;stdio.h&gt;\n#include &lt;cuda_runtime.h&gt;\n#include &lt;math.h&gt;\n#include &lt;stdlib.h&gt;\n\n#define NUM_SAMPLES 5\n#define FEATURE_DIMENSION 6\n\nvoid printMatrix(float *matrix, int row, int col) {\n    for (int r = 0; r &lt; row; r++) {\n        for (int c = 0; c &lt; col; c++) {\n            printf(\"%.3f \", matrix[r * col + c]);\n        }\n        printf(\"\\n\");\n    }\n}\n\n// Kernel: Attention Score (x = QK^T)\n__global__ void attention_score_kernel(\n    float *Q, float *K, float *x\n) {\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (row &lt; NUM_SAMPLES && col &lt; NUM_SAMPLES) {\n        float sum = 0.0f;\n        for (int i = 0; i &lt; FEATURE_DIMENSION; i++) {\n            sum += Q[row * FEATURE_DIMENSION + i] * K[col * FEATURE_DIMENSION + i];\n        }\n        x[row * NUM_SAMPLES + col] = sum;\n    }\n}\n\n// Kernel: Flash Attention\n__global__ void flash_attention_kernel(\n    float *x, float *V, float *O\n) {\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (row &lt; NUM_SAMPLES && col &lt; FEATURE_DIMENSION) {\n        float m = -INFINITY;\n        float d = 0.0f;\n        float o = 0.0f;\n\n        for (int i = 0; i &lt; NUM_SAMPLES; i++){\n            float x_val = x[row * NUM_SAMPLES + i];\n            float m_prev = m;\n            float d_prev = d;\n\n            // Compute running max and denominator\n            m = fmaxf(m_prev, x_val);\n            d = (d_prev * expf(m_prev - m)) + expf(x_val - m);\n\n            // Compute output\n            float v_val = V[i * FEATURE_DIMENSION + col];\n            o = o * ((d_prev * expf(m_prev - m)) / d) + (expf(x_val- m) / d) * v_val;\n        }\n        O[row * FEATURE_DIMENSION + col] = o;\n    }\n}\n\nvoid computeFlashAttention(\n    float *Q, float *K, float *V, float *O\n) {\n    float *d_Q, *d_K, *d_V, *d_x, *d_O;\n    size_t size_1 = NUM_SAMPLES * FEATURE_DIMENSION * sizeof(float);\n    size_t size_2 = NUM_SAMPLES * NUM_SAMPLES * sizeof(float);\n\n    // Allocate device memory\n    cudaMalloc((void**)&d_Q, size_1);\n    cudaMalloc((void**)&d_K, size_1);\n    cudaMalloc((void**)&d_V, size_1);\n    cudaMalloc((void**)&d_x, size_2);\n    cudaMalloc((void**)&d_O, size_1);\n\n    // Copy data from host to device\n    cudaMemcpy(d_Q, Q, size_1, cudaMemcpyHostToDevice);\n    cudaMemcpy(d_K, K, size_1, cudaMemcpyHostToDevice);\n    cudaMemcpy(d_V, V, size_1, cudaMemcpyHostToDevice);\n\n    // Kernel launch for attention score\n    dim3 blockDim(16, 16, 1);\n    dim3 gridDim((NUM_SAMPLES + blockDim.x - 1)/blockDim.x, (NUM_SAMPLES + blockDim.y - 1)/blockDim.y, 1);\n    attention_score_kernel&lt;&lt;&lt;gridDim, blockDim&gt;&gt;&gt;(d_Q, d_K, d_x);\n    cudaDeviceSynchronize();\n\n    // Kernel launch for flash attention\n    dim3 blockDim2(16, 16, 1);\n    dim3 gridDim2((NUM_SAMPLES + blockDim2.x - 1)/blockDim2.x, (NUM_SAMPLES + blockDim2.y - 1)/blockDim2.y, 1);\n    flash_attention_kernel&lt;&lt;&lt;gridDim2, blockDim2&gt;&gt;&gt;(d_x, d_V, d_O);\n    cudaDeviceSynchronize();\n\n    // Copy Output from device to host\n    cudaMemcpy(O, d_O, size_1, cudaMemcpyDeviceToHost);\n\n    // Free device memory\n    cudaFree(d_Q);\n    cudaFree(d_K);\n    cudaFree(d_V);\n    cudaFree(d_x);\n    cudaFree(d_O);\n}\n\nint main() {\n    float size = FEATURE_DIMENSION * NUM_SAMPLES * sizeof(float);\n    float *Q = (float *)malloc(size);\n    float *K = (float *)malloc(size);\n    float *V = (float *)malloc(size);\n    float *O = (float *)malloc(size);\n\n    // Initialize matrices\n    for (int i = 0; i &lt; NUM_SAMPLES * FEATURE_DIMENSION; i++) {\n        Q[i] = rand() % 50;\n        K[i] = rand() % 50;\n        V[i] = rand() % 50;\n    }\n    printf(\"\\nQuery:\\n\"); printMatrix(Q, NUM_SAMPLES, FEATURE_DIMENSION);\n    printf(\"\\nKey:\\n\");   printMatrix(K, NUM_SAMPLES, FEATURE_DIMENSION);\n    printf(\"\\nValue:\\n\"); printMatrix(V, NUM_SAMPLES, FEATURE_DIMENSION);\n\n    // Compute Flash Attention\n    computeFlashAttention(Q, K, V, O);\n    printf(\"\\nOutput:\\n\"); printMatrix(O, NUM_SAMPLES, FEATURE_DIMENSION);\n\n    // Free host memory\n    free(Q);\n    free(K);\n    free(V);\n    free(O);\n\n    return 0;\n}\n\n\nExplanation of Key Steps\n\nSingle-Kernel Fusion: Unlike the naive version (which had 3 separate kernels: dot-product, softmax, matmul), Flash Attention does everything in one kernel launch.\nShared Memory Usage for K and V: We load the entire 𝐾 and 𝑉 into shared memory once.\nComputing row_max and row_sum in a Streaming Fashion: We do a two-pass approach (but within the same kernel):\n\nPass 1: scan over all keys to find row_max.\nPass 2: scan again over all keys to accumulate row_sum = ∑ exp((QK)/√d – row_max). This two-pass trick avoids having to store all raw scores. We only keep track of two scalars per query.\n\nComputing Final Weighted Sum Over V: Once we know A𝑖,𝑗 we multiply by 𝑉𝑗. Since 𝑉𝑗 is already in shared memory (for that entire tile of keys), we can do this accumulation without accessing DRAM for every (𝑖,𝑗).\n\n\n\nWhy This Is “Flash” (Fast + Low Memory):\nFused Kernel means we only launch one CUDA kernel—no separate writes/reads of a big 𝑁×𝑁 attention-matrix. The partial dot products and exponentials are handled “on the fly” in registers or shared memory."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Diwakar Basnet",
    "section": "",
    "text": "AI enthusiast with a strong passion for deep learning, computer vision, and natural language processing. I enjoy exploring new technologies and advancements in the field of deep learning. Recently, I’ve started delving into CUDA and OpenAI Triton programming, further expanding my expertise and interests.\n\nExperience\nNavaNexa Tech | ML Engineer | Feb 2025 - Present\nWorked on Optimizing YOLOv8 model for QR detection and OCR tasks, coverted models to mobile suitable format.\nTreeleaf Technologies | NLP Developer | Feb 2024 - Aug 2024\nWorked on advanced AI applications, focusing on Automatic Speech Recognition (ASR) and Text-to-Speech (TTS) systems to enhance user interaction. Built and optimized a Retrieval-Augmented Generation (RAG) system for efficient information retrieval and enhanced conversational AI.\n\n\nEducation\nTribhuvan University | Kathmandu, Nepal | BSc CSIT"
  },
  {
    "objectID": "blogs.html",
    "href": "blogs.html",
    "title": "Exploring AI: Insights, Innovations and Applications",
    "section": "",
    "text": "Self-Attention\n\n\n\n\n\n\nCUDA\n\n\nTransformer\n\n\n\n\n\n\n\n\n\nJun 5, 2025\n\n\nDiwakar Basnet\n\n\n\n\n\n\n\n\n\n\n\n\nMatrix Multiplication in Triton\n\n\n\n\n\n\ntriton\n\n\n\n\n\n\n\n\n\nJan 22, 2025\n\n\nDiwakar Basnet\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blogs/matrix_multiplication/index.html",
    "href": "blogs/matrix_multiplication/index.html",
    "title": "Matrix Multiplication in Triton",
    "section": "",
    "text": "Matrix multiplication is defined for two matrices when the number of columns in the first matrix equals the number of rows in the second matrix. For example, if matrix (A) has dimensions (m x k), then matrix (B) must have dimensions (k x n) for the multiplication to be valid. The resulting matrix (C) will have dimensions (m x n).\nEach element of (C) is computed as the sum of the products of corresponding elements from a row of (A) and a column of (B). In other words, the value at position C[i][j] is obtained by multiplying each element of the i-th row of (A) with the corresponding element of the j-th column of (B), and then summing the results."
  },
  {
    "objectID": "blogs/matrix_multiplication/index.html#understanding-matmul-kernel",
    "href": "blogs/matrix_multiplication/index.html#understanding-matmul-kernel",
    "title": "Matrix Multiplication in Triton",
    "section": "Understanding matmul kernel",
    "text": "Understanding matmul kernel\nSuppose we have matrix A with dimension (M x K) and matrix B with dimension (K X N) then our resulting matrix C has dimension (M x N).\n\n@triton.jit\ndef matmul_kernel(\n        # Pointers to matrices\n        a_ptr, b_ptr, c_ptr,\n        # Matrix dimensions\n        M, N, K,\n        # The stride variables represent how much to increase the ptr by when moving by 1\n        # element in a particular dimension. E.g. `stride_am` is how much to increase `a_ptr`\n        # by to get the element one row down (A has M rows).\n        stride_am, stride_ak,  #\n        stride_bk, stride_bn,  #\n        stride_cm, stride_cn,\n        # Meta-parameters\n        BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,  #\n        GROUP_SIZE_M: tl.constexpr,  #\n        ACTIVATION: tl.constexpr  #\n):\n\n\n  Cell In[1], line 17\n    ):\n      ^\nSyntaxError: incomplete input\n\n\n\n\nThe @triton.jit decorator in Triton is used to compile a Python function as a Triton kernel allowing it to be executed efficiently in GPU. The a_ptr, b_ptr and c_ptr are the pointers to matrices A, B and C respectively. These contain the starting memory address in GPU global memory for the matrix i.e. a_ptr contains the memory address for A[0][0]. In GPU, matrices are stored in row-major order, which means that every elemets of our 2D matrix are stored in 1D memory layout. So for this reason we require stride to get next row element or column element of our matrix. stride_am represents number of elements in 1D memory layout to skip so that we obtain the element of our next row in matrix A and similarly stride_ak represents number of elements in 1D memory layout to skip so that we obtain the element of our next column in matrix A, which is usually 1.\n\n\n\n2D row-major memory layout\n\n\nThe BLOCK_SIZE_M, BLOCK_SIZE_N and BLOCK_SIZE_K are the size of our block along those axises. GROUP_SIZE_M is the maximum number of rows per group.\n\nL2 Cache optimization\n\n    # -----------------------------------------------------------\n    # Map program ids `pid` to the block of C it should compute.\n    # This is done in a grouped ordering to promote L2 data reuse.\n    # See above `L2 Cache Optimizations` section for details.\n    pid = tl.program_id(axis=0)\n    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n    group_id = pid // num_pid_in_group\n    first_pid_m = group_id * GROUP_SIZE_M\n    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n    pid_m = first_pid_m + ((pid % num_pid_in_group) % group_size_m)\n    pid_n = (pid % num_pid_in_group) // group_size_m\n\nThe num_pid_m is the number of blocks in the M axis and num_pid_n is the number of blocks in the N axis. Suppose N = 384 and BLOCK_SIZE_N = 128 then num_pid_n = ceil(384/128) = 3 i.e. there are 3 blocks in a row. Let’s consider GROUP_SIZE_M = 2 then num_pid_in_group = 2 * 3 = 6 i.e a group in our C matrix contains 6 program ids (each block is 1 pid). For a given program id we can find the group in which it belongs to by group_id = pid // num_pid_in_group. Then we calculate the starting row index in matrix A and C for the current group of thread blocks using first_pid_m = group_id * GROUP_SIZE_M.\nInstead of processing an entire matrix at once, we break our matrix into blocks. Each block fits into to the L1 cache.\n\n\n\nBlocks and group\n\n\nThe group_size_m is a runtime variable that calculates the actual number of rows a group processes, since there can be edge cases when total rows is less then GROUP_SIZE_M. The example table below shows the calculation of pid_m and pid_n for num_pid_m = 3, num_pid_n = 3, and GROUP_SIZE_M = 2. This grouping strategy is used to optimize L2 cache usage by having nearby threads work on blocks that share data.\n\n\n\npid\ngroup_id\npid_m\npid_n\n\n\n\n\n0\n0\n0\n0\n\n\n1\n0\n1\n0\n\n\n2\n0\n0\n1\n\n\n3\n0\n1\n1\n\n\n4\n0\n0\n2\n\n\n5\n0\n1\n2\n\n\n6\n1\n2\n0\n\n\n7\n1\n2\n1\n\n\n8\n1\n2\n2\n\n\n\nThreads (pid) in same group work on contiguous rows of the output matrix. For example: * pid=0 to pid=5 work on rows 0 and 1 of the output matrix. * pid=6 to pid=8 work on row 2.\nThis means that threads in the same group access nearby memory locations in the input matrices (A and B), which improves spatial locality. When one thread loads a block of data into the L2 cache, nearby threads can reuse that data, reducing the number of global memory accesses. Without grouping, threads might access disjoint regions of memory, causing frequent cache thrashing. Grouping ensures that threads in the same group access overlapping or nearby regions, reducing cache thrashing. Or simply, threads within a group compute blocks of C that are close to each other in memory, improving L2 cache utilization.\nCalculating our output matrix in grouped ordering instead of row-major ordering has an added benefit of loading fewer number of blocks into our cache as seen in the picture from official triton tutorial. Grouping also enables multiple threads to work on contiguous regions of the output matrix C, enabling efficient parallel execution.\n\n\n\nGroup ordering vs row-major ordering\n\n\n\n\nPointer Arithmetic\n\n\"\"\"Accessing blocks in matrices A and B\"\"\"\n    # ----------------------------------------------------------\n    # Create pointers for the first blocks of A and B.\n    # We will advance this pointer as we move in the K direction\n    # and accumulate\n    # `a_ptrs` is a block of [BLOCK_SIZE_M, BLOCK_SIZE_K] pointers\n    # `b_ptrs` is a block of [BLOCK_SIZE_K, BLOCK_SIZE_N] pointers\n    # See above `Pointer Arithmetic` section for details\n    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\n    offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\n    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\n\noffs_am calculates the row offsets within the current block of matrix A i.e. the block in matrix A with a certain “pid_m”. The result is taken a modulo M to wrap around if the offsets exceed the matrix dimensions. It provides the row offsets within the current 2x2 block in matrix A. Similarly, offs_bn calculates the column offsets within the 2x2 block in matrix B and offs_k calculates the column offsets in 2x2 block in matrix A and row offsets in 2x2 block in matrix B. The a_ptrs and b_ptrs calculates a 2D grid pointers to access the current block in matrix A and B respectively. a_ptrs points to a block in matrix A of size BLOCK_SIZE_M X BLOCK_SIZE_K, similarly b_ptrs points to a block in matrix B of size BLOCK_SIZE_K X BLOCK_SIZE_N.\n\n\nComputation Loop\n\n    # ------------------------------------------------------------\n    # Iterate to compute a block of the C matrix.\n    # We accumulate into a `[BLOCK_SIZE_M, BLOCK_SIZE_N]` block\n    # of fp32 values for higher accuracy.\n    # `accumulator` will be converted back to fp16 after the loop.\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n        # Load the next block of A and B, generate a mask by checking the K dimension.\n        # If it is out of bounds, set it to 0.\n        a = tl.load(a_ptrs, mask=offs_k[None, :] &lt; K - k * BLOCK_SIZE_K, other=0.0)\n        b = tl.load(b_ptrs, mask=offs_k[:, None] &lt; K - k * BLOCK_SIZE_K, other=0.0)\n        # We accumulate along the K dimension.\n        accumulator = tl.dot(a, b, accumulator)\n        # Advance the ptrs to the next K block.\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * stride_bk\n    # You can fuse arbitrary activation functions here\n    # while the accumulator is still in FP32!\n    if ACTIVATION == \"leaky_relu\":\n        accumulator = leaky_relu(accumulator)\n    c = accumulator.to(tl.float16)\n\nNow, block wise matrix multiplication is carried out according to the pid of the block. The accumulator is a block of size BLOCK_SIZE_M X BLOCK_SIZE_N, which holds the accumulated dot product of the block corresponding to C. Each thread computes its block in C by iterating over the K dimension and performing block wise multiplication of A and B. Threads in the same group access contiguous rows of A and the same columns of B.\n\n\nWriting Back in Output Matrix\n\n# -----------------------------------------------------------\n    # Write back the block of the output matrix C with masks.\n    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]\n    c_mask = (offs_cm[:, None] &lt; M) & (offs_cn[None, :] &lt; N)\n    tl.store(c_ptrs, c, mask=c_mask)\n\nThe tl.store(c_ptrs, c, mask=c_mask) stores the accumulated block multiplication into our c_ptrs location, which is calculated using the offsets and masked similar to how we calculated a_ptrs and b_ptrs."
  }
]