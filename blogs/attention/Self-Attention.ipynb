{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f066c14b-9ad0-40e5-8c39-9407467bb704",
   "metadata": {},
   "source": [
    "# title: \"Self-attention\" categories: [CUDA, Transformer] date: \"2025-06-05\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c91e0d9-00a0-44e6-ba56-172586174556",
   "metadata": {},
   "source": [
    "Since its introduction via the original transformer paper \"Attention is all you need\", self-attention has become the corner stone of many state-of-the-art deep learning models, specially in the field of NLP. Self-attention mechanisms enable models to weigh different parts of input data differently, focusing on the most relevant information while performing a task. This mimics the human ability to selectively pay attention to certain aspects of our surroundings while filtering out distractions. Attention mechanisms have been instrumental in improving the performance of various AI models, particularly in sequence-to-sequence tasks.\n",
    "\n",
    "In a transformer architecture, \"query\", \"key\" and \"value\" are the fundamental components used for calculating the self-attention. In simple terms, suppose we have a book related to animals; query represents the question that one might have such as \"what is the largest mammal on earth?\". Similarly, key represents the index or table of content in the book and value represents the actual answer that we obtain from the book, in this case \"blue whale\". In technical terms;\\n\n",
    "1. **Query:** A query is a matrix that represents the current token of request in the input sequence. Each word in the sequence has an associated query vector.\n",
    "2.  **Key:** A key is also a matrix that represents the content or identity of each token.\n",
    "3.  **Value:** A value is the actual information of each token that can be passed along.\n",
    "\n",
    "For each token in the input sequence; the self-attention model computes query vector from it, computes key and value vectors from every token in the sequence, calculates attention score by taking dot product of query and each key, applies softmax and finally computes the output as the weighted sum of the value vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62559e2b-922f-4012-9f76-962518563937",
   "metadata": {},
   "source": [
    "[![Self-attention calculation](self_attn.png)](https://sebastianraschka.com/images/blog/2023/self-attention-from-scratch/summary.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a389adc-4ddb-4098-b991-911cdb389eb8",
   "metadata": {},
   "source": [
    "## Mathematics of self-attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9680572c-434f-47d2-8042-aec963c05f45",
   "metadata": {},
   "source": [
    "### **Learnable Projections**\n",
    "Three trainable weight matrices, transform ( X ) into **queries** ( Q ), **keys** ( K ), and **values** ( V ):\n",
    "\n",
    "$$Q = X \\cdot W^Q, \\quad W^Q \\in \\mathbb{R}^{d \\times d_k}$$\n",
    "\n",
    "$$K = X \\cdot W^K, \\quad W^K \\in \\mathbb{R}^{d \\times d_k}$$\n",
    "\n",
    "$$V = X \\cdot W^V, \\quad W^V \\in \\mathbb{R}^{d \\times d_v}$$\n",
    "\n",
    "Where:\n",
    "- $( d_k )$: dimension of queries/keys\n",
    "- $( d_v )$: dimension of values\n",
    "\n",
    "### **Scaled Dot-Product Attention**\n",
    "#### **Step 1:** Compute Attention Scores\n",
    "Compute all pairwise \"compatibility\" scores between queries and keys\n",
    "$$scores_{i,j} = Q_i \\cdot K_j^T \\in \\mathbb{R}^{n \\times n}$$\n",
    "\n",
    "#### **Step 2:** Scale Scores\n",
    "Because the magnitude of dot products grows with dimension ${d_k}$, we divide by ${\\sqrt{d_k}}$:\n",
    "$$scaled_scores_{i,j} = \\frac{scores_{i,j}}{\\sqrt{d_k}}$$\n",
    "\n",
    "\n",
    "#### **Step 3:** Softmax Normalization\n",
    "For each query i, we want to convert its scores scaled_scores into a probability distribution over the N keys:\n",
    "$$A_{i,j} = softmax(scaled_scores_{i,j}), \\quad A_{i,j} = \\frac{e^{s_{i,j}}}{\\sum_{k=1}^n e^{s_{i,k}}}$$\n",
    "\n",
    "#### **Step 4:** Weighted Sum of Values\n",
    "$$Output (O_i) = A_{i,j} \\cdot V_j \\in \\mathbb{R}^{n \\times d_v}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e60cf81-8421-449e-98b0-933be8a75b93",
   "metadata": {},
   "source": [
    "## Self-attention implementation in CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b26acd-d33b-4c35-9a85-0b7c1b3680f3",
   "metadata": {},
   "source": [
    "Below is a step-by-step code walkthrough of C implementation that runs entirely on CPU.\n",
    "\n",
    "**High-level structure:**\n",
    "* Allocate and initialize input query/key/value matrices (query, key, value).\n",
    "* Compute the attention scores attentionScores = Q × Kᵀ (with loops).\n",
    "* Apply scaling + row-wise softmax to produce softmaxedScores.\n",
    "* Compute output = softmaxedScores × V.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f529652-0a94-4fed-b065-24d0f42d8ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <math.h>\n",
    "\n",
    "#define NUM_SAMPLES 2\n",
    "#define FEATURE_DIMENSION 3\n",
    "\n",
    "void printMatrix(float *matrix, int row, int col) {\n",
    "\tfor (int i = 0; i < row; i++) {\n",
    "\t\tfor (int j = 0; j < col; j++) {\n",
    "\t\t\tprintf(\"%f \", matrix[i * col + j]);\n",
    "\t\t}\n",
    "\t\tprintf(\"\\n\");\n",
    "\t}\n",
    "}\n",
    "\n",
    "// CPU Implementation of Attention\n",
    "void transposeMatrix(float *in_matrix, float *out_matrix, int row, int col) {\n",
    "\tfor (int i = 0; i < row; i++) {\n",
    "\t\tfor (int j = 0; j < col; j++) {\n",
    "\t\t\tout_matrix[j * row + i] = in_matrix[i * col + j];\n",
    "\t\t}\n",
    "\t}\n",
    "}\n",
    "\n",
    "void computeAttentionCPU(float *query, float *key, float *value,\n",
    "\t\tfloat *attentionScores, float *output) {\n",
    "\tfloat *transposeKey = (float*)malloc(FEATURE_DIMENSION * NUM_SAMPLES * sizeof(float));\n",
    "\ttransposeMatrix(key, transposeKey, NUM_SAMPLES, FEATURE_DIMENSION);\n",
    "\n",
    "\tfloat scalingFactor = 1.0f / sqrt((float)FEATURE_DIMENSION);\n",
    "\n",
    "\t// Compute attention scores\n",
    "\tfor (int i = 0; i < NUM_SAMPLES; i++) {\n",
    "\t\tfor (int j = 0; j < NUM_SAMPLES; j++) {\n",
    "\t\t\tfor (int k = 0; k < FEATURE_DIMENSION; k++) {\n",
    "\t\t\t\tattentionScores[i * NUM_SAMPLES + j] += query[i * FEATURE_DIMENSION + k] * transposeKey[k * NUM_SAMPLES + j];\n",
    "\t\t\t}\n",
    "\t\t\tattentionScores[i * NUM_SAMPLES + j] *= scalingFactor;\n",
    "\t\t}\n",
    "\t}\n",
    "\n",
    "\t// Softmax row-wise\n",
    "\tfor (int row = 0; row < NUM_SAMPLES; row++) {\n",
    "\t\tfloat maxScore = attentionScores[row * NUM_SAMPLES];\n",
    "\t\tfor (int col = 1; col < NUM_SAMPLES; col++) {\n",
    "\t\t\tif (attentionScores[row * NUM_SAMPLES + col] > maxScore) {\n",
    "\t\t\t\tmaxScore = attentionScores[row * NUM_SAMPLES + col];\n",
    "\t\t\t}\n",
    "\t\t}\n",
    "\t\tfloat sumExp = 0.0f;\n",
    "\t\tfor (int col = 0; col < NUM_SAMPLES; col++) {\n",
    "\t\t\tattentionScores[row * NUM_SAMPLES + col] = expf(attentionScores[row * NUM_SAMPLES + col] - maxScore);\n",
    "\t\t\tsumExp += attentionScores[row * NUM_SAMPLES + col];\n",
    "\t\t}\n",
    "\t\tfor (int col = 0; col < NUM_SAMPLES; col++) {\n",
    "\t\t\tattentionScores[row * NUM_SAMPLES + col] /= sumExp;\n",
    "\t\t}\n",
    "\t}\n",
    "\n",
    "\t// Multiply by value matrix\n",
    "\tfor (int i = 0; i < NUM_SAMPLES; i++) {\n",
    "\t\tfor (int j = 0; j < FEATURE_DIMENSION; j++) {\n",
    "\t\t\tfor (int k = 0; k < NUM_SAMPLES; k++) {\n",
    "\t\t\t\toutput[i * FEATURE_DIMENSION + j] += attentionScores[i * NUM_SAMPLES + k] * value[k * FEATURE_DIMENSION + j];\n",
    "\t\t\t}\n",
    "\t\t}\n",
    "\t}\n",
    "\n",
    "\tfree(transposeKey);\n",
    "}\n",
    "\n",
    "int main() {\n",
    "\tfloat query[NUM_SAMPLES * FEATURE_DIMENSION] = {\n",
    "\t\t1.0f, 0.0f, -1.0f,\n",
    "\t\t0.5f, 0.5f, 0.5f\n",
    "\t};\n",
    "\n",
    "\tfloat key[NUM_SAMPLES * FEATURE_DIMENSION] = {\n",
    "\t\t1.0f, 2.0f, 3.0f,\n",
    "\t\t4.0f, 5.0f, 6.0f\n",
    "\t};\n",
    "\t\n",
    "\tfloat value[NUM_SAMPLES * FEATURE_DIMENSION] = {\n",
    "\t\t1.0f, 1.0f, 1.0f,\n",
    "\t\t2.0f, 2.0f, 2.0f\n",
    "\t};\n",
    "\n",
    "\tfloat* output = (float*)malloc(FEATURE_DIMENSION * NUM_SAMPLES * sizeof(float));\n",
    "\tfloat* attentionScores = (float*)malloc(NUM_SAMPLES * NUM_SAMPLES * sizeof(float));\n",
    "\tcomputeAttentionCPU(query, key, value, attentionScores, output);\n",
    "\n",
    "\tprintMatrix(output, NUM_SAMPLES, FEATURE_DIMENSION);\n",
    "\n",
    "\tfree(output);\n",
    "\tfree(attentionScores);\n",
    "\n",
    "\treturn 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb1d72c-5cc5-4a3b-b3f6-0211e9fed53a",
   "metadata": {},
   "source": [
    "## Naive self-attention implementation in CUDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0f2b54-694f-4495-aeea-b9ab200e238c",
   "metadata": {},
   "source": [
    "This naive approach simply offloads the same steps to GPU, but without any fancy shared-memory tiling. Each thread computes one element of the matrix multiply or one element of the output. This is not memory- or compute-optimal, but it’s the easiest way to see how we map loops to kernels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "530b855b-2135-4945-a0da-0839f4199d9c",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid decimal literal (1963321858.py, line 22)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[2], line 22\u001b[0;36m\u001b[0m\n\u001b[0;31m    float maxScore = -1e30f;\u001b[0m\n\u001b[0m                         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid decimal literal\n"
     ]
    }
   ],
   "source": [
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "#include <math.h>\n",
    "#include <stdlib.h>\n",
    "\n",
    "#define NUM_SAMPLES 5\n",
    "#define FEATURE_DIMENSION 6\n",
    "\n",
    "void printMatrix(float *matrix, int row, int col) {\n",
    "\tfor (int i = 0; i < row; i++) {\n",
    "\t\tfor (int j = 0; j < col; j++) {\n",
    "\t\t\tprintf(\"%.3f \", matrix[i * col + j]);\n",
    "\t\t}\n",
    "\t\tprintf(\"\\n\");\n",
    "\t}\n",
    "}\n",
    "\n",
    "// Kernel: Softmax\n",
    "__global__ void softmaxKernel(float *scoreMatrix, float *softmaxMatrix) {\n",
    "\tint row = blockIdx.y * blockDim.y + threadIdx.y;\n",
    "\tif (row < NUM_SAMPLES) {\n",
    "\t\tfloat maxScore = -1e30f;\n",
    "\t\tfor (int col = 0; col < NUM_SAMPLES; ++col) {\n",
    "\t\t\tmaxScore = fmaxf(maxScore, scoreMatrix[row * NUM_SAMPLES + col]);\n",
    "\t\t}\n",
    "\t\tfloat sumExp = 0.0f;\n",
    "\t\tfor (int col = 0; col < NUM_SAMPLES; ++col) {\n",
    "\t\t\tsoftmaxMatrix[row * NUM_SAMPLES + col] = \n",
    "\t\t\t\texpf(scoreMatrix[row * NUM_SAMPLES + col] - maxScore);\n",
    "\t\t\tsumExp += softmaxMatrix[row * NUM_SAMPLES + col];\n",
    "\t\t}\n",
    "\t\tfor (int col = 0; col < NUM_SAMPLES; ++col) {\n",
    "\t\t\tsoftmaxMatrix[row * NUM_SAMPLES + col] /= sumExp;\n",
    "\t\t}\n",
    "\t}\n",
    "}\n",
    "\n",
    "// Kernel: QK^T\n",
    "__global__ void computeScoreKernel(float *queryMatrix, float *keyMatrix, float *scoreMatrix) {\n",
    "\tint row = blockIdx.y * blockDim.y + threadIdx.y;\n",
    "\tint col = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "\tif (row < NUM_SAMPLES && col < NUM_SAMPLES) {\n",
    "\t\tfloat score = 0.0f;\n",
    "\t\tfor (int d = 0; d < FEATURE_DIMENSION; ++d) {\n",
    "\t\t\tscore += queryMatrix[row * FEATURE_DIMENSION + d] *\n",
    "\t\t\t\tkeyMatrix[col * FEATURE_DIMENSION + d];\n",
    "\t\t}\n",
    "\t\tscoreMatrix[row * NUM_SAMPLES + col] = score / sqrtf(static_cast<float>(FEATURE_DIMENSION));\n",
    "\t}\n",
    "}\n",
    "\n",
    "// Kernel: Output = Softmax(QK^T) * V\n",
    "__global__ void computeOutputKernel(float * softmaxMatrix, float *valueMatrix, float *outputMatrix) {\n",
    "\tint row = blockIdx.y * blockDim.y + threadIdx.y;\n",
    "\tint col = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "\tif (row < NUM_SAMPLES && col < FEATURE_DIMENSION) {\n",
    "\t\tfloat result = 0.0f;\n",
    "\t\tfor (int k = 0; k < NUM_SAMPLES; ++k) {\n",
    "\t\t\tresult += softmaxMatrix[row * NUM_SAMPLES + k] *\n",
    "\t\t\t\tvalueMatrix[k * FEATURE_DIMENSION + col];\n",
    "\t\t}\n",
    "\t\toutputMatrix[row * FEATURE_DIMENSION + col] = result;\n",
    "\t}\n",
    "}\n",
    "\n",
    "void computeAttention(float *queryMatrix_h, float *keyMatrix_h, float *valueMatrix_h, float *attnMatrix_h) {\n",
    "\tfloat size = NUM_SAMPLES * FEATURE_DIMENSION * sizeof(float);\n",
    "\tfloat size_temp = NUM_SAMPLES * NUM_SAMPLES * sizeof(float);\n",
    "\n",
    "\tfloat *queryMatrix, *keyMatrix, *valueMatrix, *attnMatrix, *scoreMatrix, *softmaxMatrix;\n",
    "\n",
    "\t// Device memory allocation\n",
    "\tcudaMalloc((void**)&queryMatrix, size);\n",
    "\tcudaMalloc((void**)&keyMatrix, size);\n",
    "\tcudaMalloc((void**)&valueMatrix, size);\n",
    "\tcudaMalloc((void**)&attnMatrix, size);\n",
    "\tcudaMalloc((void**)&scoreMatrix, size_temp);\n",
    "\tcudaMalloc((void**)&softmaxMatrix, size_temp);\n",
    "\n",
    "\tcudaMemcpy(queryMatrix, queryMatrix_h, size, cudaMemcpyHostToDevice);\n",
    "\tcudaMemcpy(keyMatrix, keyMatrix_h, size, cudaMemcpyHostToDevice);\n",
    "\tcudaMemcpy(valueMatrix, valueMatrix_h, size, cudaMemcpyHostToDevice);\n",
    "\t\n",
    "\t// Kernel initializations\n",
    "\tdim3 blockDim(16, 16, 1);\n",
    "\tdim3 gridDim((NUM_SAMPLES+blockDim.x-1)/blockDim.x, (NUM_SAMPLES+blockDim.y-1)/blockDim.y, 1);\n",
    "\tcomputeScoreKernel<<<gridDim, blockDim>>>(queryMatrix, keyMatrix, scoreMatrix);\n",
    "\tcudaDeviceSynchronize();\n",
    "\n",
    "\tdim3 softmaxBlockDim(16, 16, 1);\n",
    "\tdim3 softmaxGridDim((NUM_SAMPLES+softmaxBlockDim.x-1)/softmaxBlockDim.x, (NUM_SAMPLES+softmaxBlockDim.y-1)/softmaxBlockDim.y, 1);\n",
    "\tsoftmaxKernel<<<softmaxGridDim, softmaxBlockDim>>>(scoreMatrix, softmaxMatrix);\n",
    "\tcudaDeviceSynchronize();\n",
    "\n",
    "\tdim3 outputBlockDim(16, 16, 1);\n",
    "\tdim3 outputGridDim((NUM_SAMPLES+outputBlockDim.x-1)/outputBlockDim.x, (NUM_SAMPLES+outputBlockDim.y-1)/outputBlockDim.y, 1);\n",
    "\tcomputeOutputKernel<<<outputGridDim, outputBlockDim>>>(softmaxMatrix, valueMatrix, attnMatrix);\n",
    "\tcudaDeviceSynchronize();\n",
    "\n",
    "\t// Copy output from device to host\n",
    "\tcudaMemcpy(attnMatrix_h, attnMatrix, size, cudaMemcpyDeviceToHost);\n",
    "\n",
    "\tcudaFree(queryMatrix);\n",
    "\tcudaFree(keyMatrix);\n",
    "\tcudaFree(valueMatrix);\n",
    "\tcudaFree(attnMatrix);\n",
    "\tcudaFree(scoreMatrix);\n",
    "\tcudaFree(softmaxMatrix);\n",
    "}\n",
    "\n",
    "int main() {\n",
    "\tint size = NUM_SAMPLES * FEATURE_DIMENSION * sizeof(float);\n",
    "\n",
    "\tfloat *queryMatrix = (float *)malloc(size);\n",
    "\tfloat *keyMatrix = (float *)malloc(size);\n",
    "\tfloat *valueMatrix = (float *)malloc(size);\n",
    "\tfloat *attnMatrix = (float *)malloc(size);\n",
    "\n",
    "\t// Initialize matrix\n",
    "\tfor (int i = 0; i < NUM_SAMPLES * FEATURE_DIMENSION; i++) {\n",
    "\t\tqueryMatrix[i] = (float)(rand() % 50);\n",
    "\t\tkeyMatrix[i] = (float)(rand() % 50);\n",
    "\t\tvalueMatrix[i] = (float)(rand() % 50);\n",
    "\t}\n",
    "\n",
    "\tprintf(\"\\nQuery:\\n\");\n",
    "\tprintMatrix(queryMatrix, NUM_SAMPLES, FEATURE_DIMENSION);\n",
    "\n",
    "\tprintf(\"\\nKey:\\n\");\n",
    "\tprintMatrix(keyMatrix, NUM_SAMPLES, FEATURE_DIMENSION);\n",
    "\n",
    "\tprintf(\"\\nValue\\n\");\n",
    "\tprintMatrix(valueMatrix, NUM_SAMPLES, FEATURE_DIMENSION);\n",
    "\n",
    "\t// Attention calculation\n",
    "\tcomputeAttention(queryMatrix, keyMatrix, valueMatrix, attnMatrix);\n",
    "\n",
    "\t// Print attention matrix\n",
    "\tprintf(\"\\nAttention matrix;\\:\\n\");\n",
    "\tprintMatrix(attnMatrix, NUM_SAMPLES, FEATURE_DIMENSION);\n",
    "\n",
    "\t// Free memory\n",
    "\tfree(queryMatrix);\n",
    "\tfree(keyMatrix);\n",
    "\tfree(valueMatrix);\n",
    "\tfree(attnMatrix);\n",
    "\n",
    "\treturn 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8edd1159-2c9c-447e-9b1f-894ba5c1b3b6",
   "metadata": {},
   "source": [
    "## Optimized self-attention in CUDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02270efd-6da2-43bf-b934-519896219153",
   "metadata": {},
   "source": [
    "This approach uses tiled/shared-memory to speed up the 𝑄𝐾⊤ and the final softmax × V operations. By loading contiguous chunks (tiles) of the input matrices into shared memory, threads within a block can cooperatively reuse data, minimizing expensive global‐memory round trips."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbc04e1b-9866-43b0-8b19-44049608c852",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid decimal literal (1695095544.py, line 35)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[3], line 35\u001b[0;36m\u001b[0m\n\u001b[0;31m    float acc = 0.0f;\u001b[0m\n\u001b[0m                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid decimal literal\n"
     ]
    }
   ],
   "source": [
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <math.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "#define NUM_SAMPLES 5\n",
    "#define FEATURE_DIMENSION 6\n",
    "#define TILE_WIDTH 16\n",
    "\n",
    "// Print utility\n",
    "void printMatrix(const float* matrix, int rows, int cols) {\n",
    "    for (int r = 0; r < rows; ++r) {\n",
    "        for (int c = 0; c < cols; ++c) {\n",
    "            printf(\"%.3f \", matrix[r * cols + c]);\n",
    "        }\n",
    "        printf(\"\\n\");\n",
    "    }\n",
    "}\n",
    "\n",
    "// Kernel: compute Q * K^T (scores)\n",
    "__global__ void scoreKernel(\n",
    "    const float* __restrict__ query,\n",
    "    const float* __restrict__ keyT,\n",
    "    float* __restrict__ score) {\n",
    "    __shared__ float sharedQ[TILE_WIDTH][TILE_WIDTH];\n",
    "    __shared__ float sharedK[TILE_WIDTH][TILE_WIDTH];\n",
    "\n",
    "    int tx = threadIdx.x;\n",
    "    int ty = threadIdx.y;\n",
    "    int bx = blockIdx.x;\n",
    "    int by = blockIdx.y;\n",
    "\n",
    "    int col = bx * TILE_WIDTH + tx;\n",
    "    int row = by * TILE_WIDTH + ty;\n",
    "    float acc = 0.0f;\n",
    "\n",
    "    int phases = (FEATURE_DIMENSION + TILE_WIDTH - 1) / TILE_WIDTH;\n",
    "    for (int p = 0; p < phases; ++p) {\n",
    "        int qCol = p * TILE_WIDTH + tx;\n",
    "        int kRow = p * TILE_WIDTH + ty;\n",
    "\n",
    "        // Load Q tile\n",
    "        if (row < NUM_SAMPLES && qCol < FEATURE_DIMENSION)\n",
    "            sharedQ[ty][tx] = query[row * FEATURE_DIMENSION + qCol];\n",
    "        else\n",
    "            sharedQ[ty][tx] = 0.0f;\n",
    "        // Load K^T tile\n",
    "        if (col < NUM_SAMPLES && kRow < FEATURE_DIMENSION)\n",
    "            sharedK[ty][tx] = keyT[kRow * NUM_SAMPLES + col];\n",
    "        else\n",
    "            sharedK[ty][tx] = 0.0f;\n",
    "        __syncthreads();\n",
    "\n",
    "        // Dot-product\n",
    "        for (int i = 0; i < TILE_WIDTH; ++i) {\n",
    "            acc += sharedQ[ty][i] * sharedK[i][tx];\n",
    "        }\n",
    "        __syncthreads();\n",
    "    }\n",
    "\n",
    "    if (row < NUM_SAMPLES && col < NUM_SAMPLES) {\n",
    "        score[row * NUM_SAMPLES + col] = acc / sqrtf((float)FEATURE_DIMENSION);\n",
    "    }\n",
    "}\n",
    "\n",
    "// Kernel: row-wise softmax\n",
    "__global__ void softmaxKernel(\n",
    "    const float* __restrict__ score,\n",
    "    float* __restrict__ softmax) {\n",
    "    int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
    "    if (row < NUM_SAMPLES) {\n",
    "        float maxv = -1e30f;\n",
    "        for (int c = 0; c < NUM_SAMPLES; ++c)\n",
    "            maxv = fmaxf(maxv, score[row * NUM_SAMPLES + c]);\n",
    "        float sum = 0.0f;\n",
    "        for (int c = 0; c < NUM_SAMPLES; ++c) {\n",
    "            float e = expf(score[row * NUM_SAMPLES + c] - maxv);\n",
    "            softmax[row * NUM_SAMPLES + c] = e;\n",
    "            sum += e;\n",
    "        }\n",
    "        for (int c = 0; c < NUM_SAMPLES; ++c)\n",
    "            softmax[row * NUM_SAMPLES + c] /= sum;\n",
    "    }\n",
    "}\n",
    "\n",
    "// Kernel: softmax * V\n",
    "__global__ void outputKernel(\n",
    "    const float* __restrict__ softmax,\n",
    "    const float* __restrict__ value,\n",
    "    float* __restrict__ output) {\n",
    "    __shared__ float sharedS[TILE_WIDTH][TILE_WIDTH];\n",
    "    __shared__ float sharedV[TILE_WIDTH][TILE_WIDTH];\n",
    "\n",
    "    int tx = threadIdx.x;\n",
    "    int ty = threadIdx.y;\n",
    "    int bx = blockIdx.x;\n",
    "    int by = blockIdx.y;\n",
    "\n",
    "    int col = bx * TILE_WIDTH + tx;\n",
    "    int row = by * TILE_WIDTH + ty;\n",
    "    float acc = 0.0f;\n",
    "\n",
    "    int phases = (NUM_SAMPLES + TILE_WIDTH - 1) / TILE_WIDTH;\n",
    "    for (int p = 0; p < phases; ++p) {\n",
    "        int sCol = p * TILE_WIDTH + tx;\n",
    "        int vRow = p * TILE_WIDTH + ty;\n",
    "\n",
    "        // Load softmax tile\n",
    "        if (row < NUM_SAMPLES && sCol < NUM_SAMPLES)\n",
    "            sharedS[ty][tx] = softmax[row * NUM_SAMPLES + sCol];\n",
    "        else\n",
    "            sharedS[ty][tx] = 0.0f;\n",
    "        // Load V tile\n",
    "        if (vRow < NUM_SAMPLES && col < FEATURE_DIMENSION)\n",
    "            sharedV[ty][tx] = value[vRow * FEATURE_DIMENSION + col];\n",
    "        else\n",
    "            sharedV[ty][tx] = 0.0f;\n",
    "        __syncthreads();\n",
    "\n",
    "        // Dot-product\n",
    "        for (int i = 0; i < TILE_WIDTH; ++i) {\n",
    "            acc += sharedS[ty][i] * sharedV[i][tx];\n",
    "        }\n",
    "        __syncthreads();\n",
    "    }\n",
    "\n",
    "    if (row < NUM_SAMPLES && col < FEATURE_DIMENSION) {\n",
    "        output[row * FEATURE_DIMENSION + col] = acc;\n",
    "    }\n",
    "}\n",
    "\n",
    "// Host helper: transpose key\n",
    "void transposeKey(const float* key, float* keyT) {\n",
    "    for (int r = 0; r < NUM_SAMPLES; ++r)\n",
    "        for (int c = 0; c < FEATURE_DIMENSION; ++c)\n",
    "            keyT[c * NUM_SAMPLES + r] = key[r * FEATURE_DIMENSION + c];\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    size_t qSize = NUM_SAMPLES * FEATURE_DIMENSION * sizeof(float);\n",
    "    size_t kTSize = NUM_SAMPLES * FEATURE_DIMENSION * sizeof(float);\n",
    "    size_t sSize = NUM_SAMPLES * NUM_SAMPLES * sizeof(float);\n",
    "\n",
    "    // Host allocations\n",
    "    float *hQ = (float*)malloc(qSize);\n",
    "    float *hK = (float*)malloc(qSize);\n",
    "    float *hV = (float*)malloc(qSize);\n",
    "    float *hKT = (float*)malloc(kTSize);\n",
    "    float *hScore = (float*)malloc(sSize);\n",
    "    float *hSoftmax = (float*)malloc(sSize);\n",
    "    float *hOut = (float*)malloc(qSize);\n",
    "\n",
    "    // Random init\n",
    "    for (int i = 0; i < NUM_SAMPLES * FEATURE_DIMENSION; ++i) {\n",
    "        hQ[i] = rand() % 50;\n",
    "        hK[i] = rand() % 50;\n",
    "        hV[i] = rand() % 50;\n",
    "    }\n",
    "\n",
    "    printf(\"\\nQuery:\\n\"); printMatrix(hQ, NUM_SAMPLES, FEATURE_DIMENSION);\n",
    "    printf(\"\\nKey:\\n\");   printMatrix(hK, NUM_SAMPLES, FEATURE_DIMENSION);\n",
    "    printf(\"\\nValue:\\n\"); printMatrix(hV, NUM_SAMPLES, FEATURE_DIMENSION);\n",
    "\n",
    "    // Transpose key on host\n",
    "    transposeKey(hK, hKT);\n",
    "\n",
    "    // Device allocations\n",
    "    float *dQ, *dKT, *dV, *dScore, *dSoftmax, *dOut;\n",
    "    cudaMalloc(&dQ, qSize);\n",
    "    cudaMalloc(&dKT, kTSize);\n",
    "    cudaMalloc(&dV, qSize);\n",
    "    cudaMalloc(&dScore, sSize);\n",
    "    cudaMalloc(&dSoftmax, sSize);\n",
    "    cudaMalloc(&dOut, qSize);\n",
    "\n",
    "    // Copy to device\n",
    "    cudaMemcpy(dQ, hQ, qSize, cudaMemcpyHostToDevice);\n",
    "    cudaMemcpy(dKT, hKT, kTSize, cudaMemcpyHostToDevice);\n",
    "    cudaMemcpy(dV, hV, qSize, cudaMemcpyHostToDevice);\n",
    "\n",
    "    // Launch score kernel\n",
    "    dim3 block(TILE_WIDTH, TILE_WIDTH);\n",
    "    dim3 gridScore((NUM_SAMPLES+TILE_WIDTH-1)/TILE_WIDTH,\n",
    "                   (NUM_SAMPLES+TILE_WIDTH-1)/TILE_WIDTH);\n",
    "    scoreKernel<<<gridScore, block>>>(dQ, dKT, dScore);\n",
    "    cudaDeviceSynchronize();\n",
    "\n",
    "    // Softmax kernel\n",
    "    dim3 gridSm((NUM_SAMPLES+TILE_WIDTH-1)/TILE_WIDTH, 1);\n",
    "    softmaxKernel<<<gridSm, block>>>(dScore, dSoftmax);\n",
    "    cudaDeviceSynchronize();\n",
    "\n",
    "    // Output kernel\n",
    "    dim3 gridOut((FEATURE_DIMENSION+TILE_WIDTH-1)/TILE_WIDTH,\n",
    "                 (NUM_SAMPLES+TILE_WIDTH-1)/TILE_WIDTH);\n",
    "    outputKernel<<<gridOut, block>>>(dSoftmax, dV, dOut);\n",
    "    cudaDeviceSynchronize();\n",
    "\n",
    "    // Copy back\n",
    "    cudaMemcpy(hOut, dOut, qSize, cudaMemcpyDeviceToHost);\n",
    "\n",
    "    printf(\"\\nAttention Output:\\n\");\n",
    "    printMatrix(hOut, NUM_SAMPLES, FEATURE_DIMENSION);\n",
    "\n",
    "    // Cleanup\n",
    "    free(hQ); free(hK); free(hV); free(hKT);\n",
    "    free(hScore); free(hSoftmax); free(hOut);\n",
    "    cudaFree(dQ); cudaFree(dKT); cudaFree(dV);\n",
    "    cudaFree(dScore); cudaFree(dSoftmax); cudaFree(dOut);\n",
    "\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0934b9dc-ec0b-47bc-8ccb-0389e15245e8",
   "metadata": {},
   "source": [
    "## Flash attention in CUDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb9646a-89dc-489a-84c6-458a708f0848",
   "metadata": {},
   "source": [
    "Flash Attention is a specialized, fused‐kernel approach that was introduced to compute softmax(𝑄𝐾⊤)𝑉 in a single (or a small number of) GPU kernels, without ever storing the full 𝑁×𝑁 attention matrix in DRAM. Instead, it computes partial dot products and partial softmaxes in registers or shared memory, blocking in both the 𝑁 (sequence length) and 𝑑 (feature) dimensions. This greatly reduces memory bandwidth and peak memory usage, making it possible to handle very large sequences on a single GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734dd63d-ac97-4158-a1c4-5d9d0f8fe91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "#include <math.h>\n",
    "#include <stdlib.h>\n",
    "\n",
    "#define NUM_SAMPLES 5\n",
    "#define FEATURE_DIMENSION 6\n",
    "\n",
    "void printMatrix(float *matrix, int row, int col) {\n",
    "    for (int r = 0; r < row; r++) {\n",
    "        for (int c = 0; c < col; c++) {\n",
    "            printf(\"%.3f \", matrix[r * col + c]);\n",
    "        }\n",
    "        printf(\"\\n\");\n",
    "    }\n",
    "}\n",
    "\n",
    "// Kernel: Attention Score (x = QK^T)\n",
    "__global__ void attention_score_kernel(\n",
    "    float *Q, float *K, float *x\n",
    ") {\n",
    "    int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
    "    int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "\n",
    "    if (row < NUM_SAMPLES && col < NUM_SAMPLES) {\n",
    "        float sum = 0.0f;\n",
    "        for (int i = 0; i < FEATURE_DIMENSION; i++) {\n",
    "            sum += Q[row * FEATURE_DIMENSION + i] * K[col * FEATURE_DIMENSION + i];\n",
    "        }\n",
    "        x[row * NUM_SAMPLES + col] = sum;\n",
    "    }\n",
    "}\n",
    "\n",
    "// Kernel: Flash Attention\n",
    "__global__ void flash_attention_kernel(\n",
    "    float *x, float *V, float *O\n",
    ") {\n",
    "    int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
    "    int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "\n",
    "    if (row < NUM_SAMPLES && col < FEATURE_DIMENSION) {\n",
    "        float m = -INFINITY;\n",
    "        float d = 0.0f;\n",
    "        float o = 0.0f;\n",
    "\n",
    "        for (int i = 0; i < NUM_SAMPLES; i++){\n",
    "            float x_val = x[row * NUM_SAMPLES + i];\n",
    "            float m_prev = m;\n",
    "            float d_prev = d;\n",
    "\n",
    "            // Compute running max and denominator\n",
    "            m = fmaxf(m_prev, x_val);\n",
    "            d = (d_prev * expf(m_prev - m)) + expf(x_val - m);\n",
    "\n",
    "            // Compute output\n",
    "            float v_val = V[i * FEATURE_DIMENSION + col];\n",
    "            o = o * ((d_prev * expf(m_prev - m)) / d) + (expf(x_val- m) / d) * v_val;\n",
    "        }\n",
    "        O[row * FEATURE_DIMENSION + col] = o;\n",
    "    }\n",
    "}\n",
    "\n",
    "void computeFlashAttention(\n",
    "    float *Q, float *K, float *V, float *O\n",
    ") {\n",
    "    float *d_Q, *d_K, *d_V, *d_x, *d_O;\n",
    "    size_t size_1 = NUM_SAMPLES * FEATURE_DIMENSION * sizeof(float);\n",
    "    size_t size_2 = NUM_SAMPLES * NUM_SAMPLES * sizeof(float);\n",
    "\n",
    "    // Allocate device memory\n",
    "    cudaMalloc((void**)&d_Q, size_1);\n",
    "    cudaMalloc((void**)&d_K, size_1);\n",
    "    cudaMalloc((void**)&d_V, size_1);\n",
    "    cudaMalloc((void**)&d_x, size_2);\n",
    "    cudaMalloc((void**)&d_O, size_1);\n",
    "\n",
    "    // Copy data from host to device\n",
    "    cudaMemcpy(d_Q, Q, size_1, cudaMemcpyHostToDevice);\n",
    "    cudaMemcpy(d_K, K, size_1, cudaMemcpyHostToDevice);\n",
    "    cudaMemcpy(d_V, V, size_1, cudaMemcpyHostToDevice);\n",
    "\n",
    "    // Kernel launch for attention score\n",
    "    dim3 blockDim(16, 16, 1);\n",
    "    dim3 gridDim((NUM_SAMPLES + blockDim.x - 1)/blockDim.x, (NUM_SAMPLES + blockDim.y - 1)/blockDim.y, 1);\n",
    "    attention_score_kernel<<<gridDim, blockDim>>>(d_Q, d_K, d_x);\n",
    "    cudaDeviceSynchronize();\n",
    "\n",
    "    // Kernel launch for flash attention\n",
    "    dim3 blockDim2(16, 16, 1);\n",
    "    dim3 gridDim2((NUM_SAMPLES + blockDim2.x - 1)/blockDim2.x, (NUM_SAMPLES + blockDim2.y - 1)/blockDim2.y, 1);\n",
    "    flash_attention_kernel<<<gridDim2, blockDim2>>>(d_x, d_V, d_O);\n",
    "    cudaDeviceSynchronize();\n",
    "\n",
    "    // Copy Output from device to host\n",
    "    cudaMemcpy(O, d_O, size_1, cudaMemcpyDeviceToHost);\n",
    "\n",
    "    // Free device memory\n",
    "    cudaFree(d_Q);\n",
    "    cudaFree(d_K);\n",
    "    cudaFree(d_V);\n",
    "    cudaFree(d_x);\n",
    "    cudaFree(d_O);\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    float size = FEATURE_DIMENSION * NUM_SAMPLES * sizeof(float);\n",
    "    float *Q = (float *)malloc(size);\n",
    "    float *K = (float *)malloc(size);\n",
    "    float *V = (float *)malloc(size);\n",
    "    float *O = (float *)malloc(size);\n",
    "\n",
    "    // Initialize matrices\n",
    "    for (int i = 0; i < NUM_SAMPLES * FEATURE_DIMENSION; i++) {\n",
    "        Q[i] = rand() % 50;\n",
    "        K[i] = rand() % 50;\n",
    "        V[i] = rand() % 50;\n",
    "    }\n",
    "    printf(\"\\nQuery:\\n\"); printMatrix(Q, NUM_SAMPLES, FEATURE_DIMENSION);\n",
    "    printf(\"\\nKey:\\n\");   printMatrix(K, NUM_SAMPLES, FEATURE_DIMENSION);\n",
    "    printf(\"\\nValue:\\n\"); printMatrix(V, NUM_SAMPLES, FEATURE_DIMENSION);\n",
    "\n",
    "    // Compute Flash Attention\n",
    "    computeFlashAttention(Q, K, V, O);\n",
    "    printf(\"\\nOutput:\\n\"); printMatrix(O, NUM_SAMPLES, FEATURE_DIMENSION);\n",
    "\n",
    "    // Free host memory\n",
    "    free(Q);\n",
    "    free(K);\n",
    "    free(V);\n",
    "    free(O);\n",
    "\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a650e887-ea84-4288-a9e8-9406200c69f5",
   "metadata": {},
   "source": [
    "#### **Explanation of Key Steps**\n",
    "* **Single-Kernel Fusion:** Unlike the naive version (which had 3 separate kernels: dot-product, softmax, matmul), Flash Attention does everything in one kernel launch.\n",
    "* **Shared Memory Usage for K and V:** We load the entire 𝐾 and 𝑉 into shared memory once.\n",
    "* **Computing row_max and row_sum in a Streaming Fashion:** We do a two-pass approach (but within the same kernel):\n",
    "    * Pass 1: scan over all keys to find row_max.\n",
    "    * Pass 2: scan again over all keys to accumulate row_sum = ∑ exp((QK)/√d – row_max).\n",
    "    This two-pass trick avoids having to store all raw scores. We only keep track of two scalars per query.\n",
    "* **Computing Final Weighted Sum Over V:** Once we know A𝑖,𝑗 we multiply by 𝑉𝑗. Since 𝑉𝑗 is already in shared memory (for that entire tile of keys), we can do this accumulation without accessing DRAM for every (𝑖,𝑗).\n",
    "\n",
    "#### **Why This Is “Flash” (Fast + Low Memory):**\n",
    "Fused Kernel means we only launch one CUDA kernel—no separate writes/reads of a big 𝑁×𝑁 attention-matrix.\n",
    "The partial dot products and exponentials are handled “on the fly” in registers or shared memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c5872f-7e6d-4b13-a80d-88288cc58649",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
